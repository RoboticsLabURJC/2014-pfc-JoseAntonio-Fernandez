%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Plantilla de memoria en LaTeX para la ETSIT - Universidad Rey Juan Carlos
%%
%% Por Gregorio Robles <grex arroba gsyc.urjc.es>
%%     Grupo de Sistem\'as y Comunicaciones
%%     Escuela T\'ecnica Superior de Ingenieros de Telecomunicaci\'on
%%     Universidad Rey Juan Carlos
%% (muchas ideas tomadas de Internet, colegas del GSyC, antiguos alumnos...
%%  etc. Muchas gracias a todos)
%%
%% La \'ultima versi\'on de esta plantilla est\'a siempre disponible en:
%%     https://github.com/gregoriorobles/plantilla-memoria
%%
%% Para obtener PDF, ejecuta en la shell:
%%   make
%% (las im\'agenes deben ir en PNG o JPG)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper, 12pt]{book}
\usepackage[T1]{fontenc}

\usepackage[a4paper, left=2.5cm, right=2.5cm, top=3cm, bottom=3cm]{geometry}
\usepackage{times}
\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel} % Comenta esta l\'inea si tu memoria es en ingl\'es
\usepackage{url}
\usepackage{nicefrac}
%\usepackage[dvipdfm]{graphicx}
\usepackage{graphicx}
\usepackage{float}  %% H para posicionar figuras
\usepackage[nottoc, notlot, notlof, notindex]{tocbibind} %% Opciones de \'indice
\usepackage{latexsym}  %% Logo LaTeX
\usepackage{eurosym}
\usepackage{subfig}
\usepackage{siunitx}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray97}{gray}{.97}
\definecolor{gray75}{gray}{.75}
\definecolor{gray45}{gray}{.45}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\scriptsize,
otherkeywords={self},             % Add keywords here
keywordstyle=\scriptsize\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\scriptsize\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=single,                         % Any extra options here
showstringspaces=false            % 
framerule=0pt,
aboveskip=0.5cm,
framextopmargin=3pt,
framexbottommargin=3pt,
framexleftmargin=0.4cm,
framesep=0pt,
rulesep=.4pt,
backgroundcolor=\color{gray97},
rulesepcolor=\color{black},
commentstyle=\color{gray45},
numbers=left,
numbersep=15pt,
numberstyle=\scriptsize,
numberfirstline = false,
breaklines=true,
}}


% Python style for highlighting
\newcommand\pythonstyleinline{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=Ltb,                         % Any extra options here
showstringspaces=false            % 
framerule=0pt,
aboveskip=0.5cm,
framextopmargin=3pt,
framexbottommargin=3pt,
framexleftmargin=0.4cm,
framesep=0pt,
rulesep=.4pt,
backgroundcolor=\color{gray97},
rulesepcolor=\color{black},
commentstyle=\color{gray45},
numbers=left,
numbersep=15pt,
numberstyle=\ttm,
numberfirstline = false,
breaklines=true,
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyleinline\lstinline!#1!}}




\title{Navegación por posición para un avión autónomo con JdeRobot}
\author{Jos\'e Antonio Fern\'andez Casillas}

\renewcommand{\baselinestretch}{1.5}  %% Interlineado

\begin{document}

\renewcommand{\refname}{Bibliograf\'ia}  %% Renombrando
\renewcommand{\appendixname}{Ap\'endice}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PORTADA

\begin{titlepage}
\begin{center}
\begin{tabular}[c]{c c}
%\includegraphics[bb=0 0 194 352, scale=0.25]{logo} &
\includegraphics[scale=0.25]{img/logo_vect.png} &
\begin{tabular}[b]{l}
\Huge
\textsf{UNIVERSIDAD} \\
\Huge
\textsf{REY JUAN CARLOS} \\
\end{tabular}
\\
\end{tabular}

\vspace{3cm}

\Large
INGENIER\'IA T\'ECNICA INFORM\'ATICA EN SISTEMAS

\Large
INGENIER\'IA T\'ECNICA EN INFORM\'ATICA DE SISTEMAS

\vspace{0.4cm}

\large
Curso Acad\'emico 2016/2017

\vspace{0.8cm}

Proyecto Fin de Carrera

\vspace{2.5cm}

\LARGE
Navegación por posición para un avión autónomo con JdeRobot

\vspace{4cm}

\large
Autor : Jos\'e Antonio Fern\'andez Casillas \\
Tutor : Jos\'e Mar\'ia Cañas Plaza
\end{center}
\end{titlepage}

\newpage
\mbox{}
\thispagestyle{empty} % para que no se numere esta pagina


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Para firmar
\clearpage
\pagenumbering{gobble}
\chapter*{}

\vspace{-4cm}
\begin{center}
\LARGE
\textbf{Proyecto Fin de Carrera}

\vspace{1cm}
\large
Navegación por posición para un avión autónomo con JdeRobot

\vspace{1cm}
\large
\textbf{Autor :} Jos\'e Antonio Fern\'andez Casillas
\textbf{Tutor :} Jos\'e Mar\'ia Cañas Plaza

\end{center}

\vspace{1cm}
La defensa del presente Proyecto Fin de Carrera se realiz\'o el d\'ia \qquad$\;\,$ de \qquad\qquad\qquad\qquad \newline de 2017, siendo calificada por el siguiente tribunal:


\vspace{0.5cm}
\textbf{Presidente:}

\vspace{1.2cm}
\textbf{Secretario:}

\vspace{1.2cm}
\textbf{Vocal:}


\vspace{1.2cm}
y habiendo obtenido la siguiente calificaci\'on:

\vspace{1cm}
\textbf{Calificaci\'on:}


\vspace{1cm}
\begin{flushright}
Fuenlabrada, a \qquad$\;\,$ de \qquad\qquad\qquad\qquad de 2017
\end{flushright}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Dedicatoria

\chapter*{}
\pagenumbering{Roman} % para comenzar la numeracion de paginas en numeros romanos
\begin{flushright}
\textit{A mis padres y a mi mujer,\\ sin vuestra perseverancia,\\ apoyo y paciencia\\ nada de esto\\ hubiese sido posible.\\A mi hijo Pablo}
\end{flushright}


\chapter*{Resumen}
%\addcontentsline{toc}{chapter}{Resumen} % si queremos que aparezca en el \'indice
\markboth{RESUMEN}{RESUMEN} % encabezado

Los UAV o Drones se han popularizado en los \'ultimos años hasta el punto de formar parte de nuestro d\'ia a d\'ia con aplicaciones en muchos ámbitos de nuestra vida. 
El objetivo de este trabajo final es desarrollar tecnolog\'ia que permita la programaci\'on de aplicaciones rob\'oticas con un avi\'on aut\'onomo. Este proyecto se ubica en el contexto de rob\'otica y drones, en rob\'otica a\'erea.
En él se documenta el análisis, diseño e implementación de un driver y una aplicación de teleoperación por puntos de paso en JdeRobot. A lo largo del mismo se detallan tanto el hardware como el software utilizados para la implementación y se describen los puntos más importantes. 
Como plataforma robótica se ha empleado JdeRobot 5.3. El lenguaje de programación ha sido Python y se han empleado las bibliotecas OpenCV, OWSLib y PyQt, para tratamiento de imágenes, apoyo a la descarga de mapas y para construir la interfaz gráfica de usuario de la aplicación.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \'iNDICES %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Las buenas noticias es que los \'indices se generan autom\'aticamente.
% Lo \'unico que tienes que hacer es elegir cu\'ales quieren que se generen,
% y comentar/descomentar esa instrucci\'on de LaTeX.

%%%% \'indice de contenidos
\tableofcontents 
%%%% \'indice de figuras
\cleardoublepage
%\addcontentsline{toc}{chapter}{Lista de figuras} % para que aparezca en el indice de contenidos
\listoffigures % indice de figuras
%%%% \'indice de tablas
%\cleardoublepage
%\addcontentsline{toc}{chapter}{Lista de tablas} % para que aparezca en el indice de contenidos
%\listoftables % indice de tablas


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCCI\'oN %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Introducci\'on}
\label{sec:intro} % etiqueta para poder referenciar luego en el texto con ~\ref{sec:intro}
\pagenumbering{arabic} % para empezar la numeraci\'on de p\'agina con n\'umeros

Los UAV o Drones se han popularizado en los \'ultimos años hasta el punto de formar parte de nuestro d\'ia a d\'ia con aplicaciones en muchos ámbitos de nuestra vida. Si bien se est\'an utilizando ya de forma habitual en sectores como el cine o la ingenier\'ia civil, a\'un se est\'an explorando muchas de las posibles utilidades que estos robots pueden ofrecer.

El objetivo de este trabajo es desarrollar tecnolog\'ia que permita la programaci\'on de aplicaciones rob\'oticas con un avi\'on aut\'onomo. Este proyecto se ubica por lo  tanto en el contexto de rob\'otica y drones, en rob\'otica a\'erea.

\section{Rob\'otica A\'erea}
\label{sec:robotica_aerea}

Los or\'igenes de la rob\'otica aérea tienen origen militar y su avance ha estado intr\'insecamente ligado a este \'ambito durante todo el siglo XX. Se consideran el origen de los aviones no tripulados los experimentos llevados a cabo a principios del siglo XX durante la 1ª guerra mundial como el \textit{``Aerial Target''} desarrollado por el capit\'an A. H. Low para su uso como blanco a\'ereo. Si bien eran veh\'iculos no tripulados (\textit{Unmaned Aereal Vehicles}) no eran aut\'onomos y eran manejados desde tierra a trav\'es de una radio. 
No es hasta el final del siglo XX cuando bajo el escenario de la guerra de Vietnam y ante la creciente p\'erdida de vidas de los pilotos, estos veh\'iculos vuelven de nuevo a ser objeto de desarrollo y se conviertan en veh\'iculos aut\'onomos.

Desde ese momento y hasta nuestros d\'ias se utilizan de forma habitual en el \'ambito militar en misiones de reconocimiento, bombardeos o apoyo sin arriesgar vidas humanas.

A los largo de los primeros años de este siglo, debido al abaratamiento de los componentes electr\'onicos y a su minituarizaci\'on y potencia, la rob\'otica a\'erea se ha 'desmilitarizado' y est\'a experimentado un enorme crecimiento en aplicaciones civiles.

Hoy en d\'ia es com\'un encontrar en cualquier jugueter\'ia quadrac\'opetros radio-pilotados por poco menos de 30 euros y en tiendas especializadas podemos encontrarlos ya con el hardware y software integrados que les permiten seguir una serie de puntos de control y comportarse de forma aut\'onoma por poco m\'as de 200 \euro.

El uso de AUV o drones se ha popularizado tanto que es una de las industrias en las que m\'as ha crecido la inversi\'on, y es que, seg\'un la empresa analista especializada en drones Droneii con sede en Hamburgo en un estudio sobre la inversi\'on en el sector\cite{Droneii:_drone_Invest}, en europa se invirti\'o en proyectos dom\'esticos en 2016 cerca de 65 millones de d\'olares increment\'andose esta cifra hasta los 314 millones si atendemos al mercado norteamericano.

Estos datos se asientan en un mercado cada vez m\'as extendido y con una gran proyecci\'on de crecimiento. La publicaci\'on BI Intelligence\footnote{\url{http://www.businessinsider.com}}\cite{businessinsider:_drones_5_years} espera que las ventas de drones alcancen los 12.000 millones en 2021.
La venta de drones es s\'olo una de las piezas de este negocio incipiente, empresas como DJI, Xiaomy o 3DR están en la vanguardiaa de la  innovaci\'on de \'estos, pero es tan solo la punta del iceberg. Esta industria est\'a potenciando otras como la de las videoc\'amaras deportivas GoPRO e incluso están surgiendo nuevos puestos de trabajo como el de operador de drones.
Las grandes empresas tecnol\'ogicas ven el potencial econ\'omico que pueden aportar a sus balances y se est\'a produciendo una pugna por adquirir las principales empresas esecializadas en drones, Verizon compr\'o en febrero Skyward, FaceBook compr\'o Acenta y Google Titan Aeroespace.

La enorme aceptaci\'on y expansi\'on de los drones se ha producido de un modo tan explosivo que en ciertos aspectos de la sociedad no se ha avanzado lo suficiente como para que su utilizaci\'on se haga de forma segura.

\begin{itemize}
\item Problemas de seguridad. Los drones y en especial los drones de gran envergadura, m\'as all\'a de su uso profesional o l\'udico, pueden ser muy peligrosos. Sus palas giran a m\'as de 1000RPM y pueden producir cortes o amputaciones o producir daños personales y/o materiales en el caso de una p\'erdida de control del mismo o una ca\'ida. Esto se hizo patente cuando durante la filmaci\'on de un concierto del cantante Enrique Iglesias en Tijuana (M\'exico) \'este agarr\'o el drone que le grababa de forma espont\'anea y \'esto le produjo severos cortes en su mano derecha que hizo que sangrase profusamente.

Tambi\'en pueden utilizarse con fines terroristas, como se sospech\'o en Francia cuando se detectaron en octubre de 2014 volando en las proximidades de varias centrales nucleares. Adem\'as, en la noche del 19 al 20 de enero de 2015 otro drone sobrevol\'o el Palacio del El\'iseo, donde tiene su residencia el presidente de la rep\'ublica francesa. Y a ellos hay que sumar los 19 drones que han sido avistados sobrevolando 17 centrales nucleares francesas de octubre a febrero de 2015.
Su bajo coste y su versatilidad producen que proliferen muchos drones construidos para fines oscuros.

Algunas empresas del sector se han hecho eco de estos ataques y han deshabilitado sus drones en zonas de conflicto para evitar que se utilicen como armas de guerra como es el caso de la empresa china DJI, l\'ider en el sector.\footnote{\url{http://clipset.20minutos.es/dji-bloquea-drones-guerra-irak-siria/}}

A estos problemas de seguridad hay que unir el problema del \textit{hacking}. Muchos de los m\'as extendidos drones comerciales no tienen protecci\'on alguna contra el ataque de un ciberdelincuente. Por poner un ejemplo, el modo en el que el Parrot AR-Drone se empareja con el m\'ovil es una conexi\'on plana sin contraseña, hace posible conectarse a \'el por un tercero con unas pocas l\'ineas de c\'odigo\footnote{Gu\'ia sencilla de hacking del AR-Drone 2.0 - http://www.xdrones.es/guia-para-hackear-el-ar-drone-2-0/}\footnote{C\'omo atacar el AR-Drone 2.0 con nodecopter - http://www.nodecopter.com/hack}\footnote{Entorno para \textit{hackear} drones a trav\'es de WIFI - https://github.com/samyk/skyjack}.

Existen t\'ecnicas como el \textit{GPS Spoofing} que pueden ``secuestrar'' cualquier drone que se gu\'ie por GPS como ocurri\'o en Ir\'an en 2011 d\'onde hackers iran\'ies se hicieron con el control de un drone militar americano Lockheed Martin RQ-170 Sentinel\footnote{\url{https://en.wikipedia.org/wiki/Iran\%E2\%80\%93U.S._RQ-170_incident}} mediante \'esta t\'ecnica que engaña a la antena GPS del UAV haci\'endole pensar que se encuentra en otro lugar. Si bien para este ataque se necesit\'o un hardware caro que har\'ia el ataque prohibitivo, hoy es posible realizarlo por unos pocos cientos de euros con placas como hackrf o braderf como se demostr\'o por la compañ\'ia china Qihoo 360 en las conferencias de Haking DEFCON de 2015.\cite{rtl-sdr:_gps_spoofing}

\item Problemas regulatorios. Otro problema importante que apenas se est\'a empezando a abordar es el regulatorio. Con el fin de minimizar el riesgo para las personas se est\'a llevando a cabo una regulaci\'on del sector en todo el mundo. En España por ejemplo hasta abril de 2014, volar un drone para uso civil estaba absolutamente prohibido. La no regulaci\'on no indicaba que se pudiese volar, indicaba m\'as bien todo lo contrario.
En este marco regulatorio no se puede por ejemplo, volar un drone en n\'ucleos urbanos, s\'olo debe hacerse en zonas previstas a tal efecto. Tampoco es posible volar un drone de m\'as de 2Kg m\'as all\'a de donde puedas verlo (500m), y es que antes era muy usual ponerle una c\'amara y volar varios kil\'ometros con \'el.

Para hacernos una idea clara del marco regulatorio actual para su uso no profesional basta con fijarnos en la siguiente Figura
1.1 publicada por la agencia EFE.
\begin{figure}
  \centering
  \includegraphics[scale=0.55]{img/infografia_legislacion_drones.jpg}
  \caption{Infograf\'ia de la Agencia EFE con la regulaci\'on}
  \label{fig:infografia_efe}
\end{figure}

Para poder utilizar los drones con fines profesionales la ley es a\'un m\'as restrictiva. La Agencia Española de Seguridad A\'erea AESA exige el registro de las aeronaves civiles pilotadas por control remoto cuya masa m\'axima al despegue exceda de 25 kg, que deber\'an estar inscritas en el Registro de matr\'icula de aeronaves y disponer de certificado de aeronavegabilidad.
Para drones de menor peso, el piloto deber\'a presentar ante la Agencia Estatal de Seguridad A\'erea una comunicaci\'on previa y declaraci\'on responsable con una antelaci\'on m\'inima de cinco d\'ias al d\'ia del inicio de la operaci\'on, y \'este ha de estar habilitado como operador de drones. La ley completa se puede consultar el BOE\footnote{\url{ http://www.seguridadaerea.gob.es/media/4243006/rdl_8_2014_4julio.pdf}}

\end{itemize}

Con su uso ya ampliamente extendido en sectores como el cine, la televisi\'on, fotograf\'ia, agropecuario, forestal, ingenier\'ia civil y presencia en sectores como el de salvamento o seguridad y protecci\'on, el nicho de mercado de los UAV está lejos de su cima. D\'ia a d\'ia se investigan nuevos usos en sectores como el de la log\'istica o las telecomunicaciones, como veremos m\'as adelante, suponiendo un reto constante para investigadores, desarrolladores, ingenieros, inversores y entidades regulatorias.



\section{Tipos de Aeronaves}
\label{sec:tipos_aeronaves}


Las aeronaves son la base que permite que nuestro robot vuele, de ah\'i que convenga dedicar unas l\'ineas a entender los fundamentos de las mismas y en particular las que son objeto de estudio y desarrollo en este PFC, los llamados aerodinos.

Los aerodinos son aquellas aeronaves que vuelan a pesar de pesar m\'as que el aire, son capaces de generar sustentaci\'on por sus propios medios a diferencia de los aerostatos como por ejemplo los globos aerost\'aticos. Existen principalmente 2 tipos de aerodinos si atendemos al modo en que generan su sustentaci\'on con sus alas: de ala fija y las de ala rotatoria.

Dentro de los primeros tenemos aquellos aerodinos que tienes sus alas fijas al fuselaje, y que comúnnmente conocemos como aviones. Seg\'un la OACI, un avi\'on es un «Aerodino propulsado por motor, que debe su sustentaci\'on en vuelo principalmente a reacciones aerodin\'amicas ejercidas sobre superficies que permanecen fijas en determinadas condiciones de vuelo».
Algunos ejemplos de aerodinos de ala fija son los aeroplanos, planeadores/veleros, aladeltas, parapentes, paramotores y ultraligeros.

Este tipo de aerodinos tienen como principal ventaja que la carga de aire que necesitan en sus alas puede ser producida de muchas formas distintas (los veleros no tienen ning\'un tipo de propulsi\'on). Esta carga es variable en funci\'on de la superficie alar del mismo y permite por tanto cargas m\'as grandes que si instal\'asemos el mismo propulsor en un ala rotatoria.
Pongamos como ejemplo el A380 de Airbus, es el avi\'on de pasajeros m\'as grande del mundo y cuenta con 4 motores que producen un empuje de entre 70.000 y 80.000lbs, unas 32-36 toneladas de empuje cada uno generando por  tanto entre los 4 a m\'aximo rendimiento y optim\'as condiciones alrededor de 144 toneladas de empuje. Este avi\'on tiene un peso m\'aximo al despegue\footnote{Peso m\'aximo que es capaz de soportar un avi\'on en su maniobra de despegue}\ de entre 560 y 590 toneladas. Tenemos por tanto que necesitamos en este caso \nicefrac{1}{4} del peso total en empuje para despegar este avi\'on.
Si hici\'esemos este mismo ejercicio con un aerodino de ala rotatoria como el Boing AH-64 o Apache con un peso m\'aximo al despegue de 9,5 toneladas necesitar\'iamos que la combinaci\'on que realizan empuje y palas superase esos 9,5 toneladas para siguiera levantar del suelo.
Este tipo de aerodinos son por tanto m\'as eficientes, r\'apidos, con mayor carga de pago, mayor alcance debido a su menor consumo y m\'as estables.

Dentro de la tipificaci\'on de ala rotatoria tenemos aquellos aerodinos que producen su sustentaci\'on con el movimiento (rotaci\'on) de sus alas. En este tipo de aerodinos las alas, tambi\'en llamadas 'palas', que giran en torno a un eje produciendo con este giro la sustentaci\'on necesaria para despegar del suelo.
Algunos ejemplos de este tipo de aerodinos son los helic\'opteros, autogiros, convertibles o los, ampliamente conocidos en rob\'otica aérea, quadric\'opteros.
Este tipo de aerodino tiene como principal ventaja frente a los ala fija en su versatilidad a la hora de realizar las maniobras de despegue y aterrizaje que pueden realizarse de forma vertical (VTOL\footnote{Vertical take off and landing }), adem\'as de la capacidad de realizar vuelo estacionario\footnote{Mantenerse est\'aticamente en un punto elevado} que le hacen imprescindible en escenarios poco accesibles o donde no es posible aterrizar, como el rescate mar\'itimo.


\section{Aplicaciones}
\label{sec:aplicaciones}

La rob\'otica a\'erea ha experimentado un crecimiento exponencial en los \'ultimos años, se ha popularizado su uso y se ha extendido la comercializaci\'on de drones.

El sector donde m\'as r\'apida acogida ha tenido la rob\'otica a\'erea ha sido el sector audiovisual, se usa de forma habitual en cine, grabaci\'on de espect\'aculos en directo televisi\'on y fotograf\'ia. El porqu\'e de tal acogida se basa principalmente en dos factores, los costes y la viabilidad t\'ecnica. 
Los costes de realizar una toma a\'erea en una producci\'on antes pasaban por el alquiler de un helic\'optero, dependiendo de la toma, as\'i como del material y la contrataci\'on de los medios humanos a bordo para realizarlas pod\'ia ascender a entre 4000 y 6000 euros la hora. Hoy en d\'ia basta con un pequeño drone de entre 400 y 1800 euros\footnote{Precios aproximados extra\'idos de la empresa especializada World Aviation Helicopters \url{https://www.worldaviation.es/es/servicio-drones.aspx}} por jornada, e incluir\'ian en el tramo m\'as elevado piloto y operador de c\'amara.
Como se puede constatar f\'acilmente, el ahorro no es desdeñable y ofrece a pequeñas productoras acceder a este tipo de grabaciones que de otro modo ser\'ian privativas.
Aunque el aspecto econ\'omico es especialmente importante para decantarse por el uso de este tipo de medios, existen en el mundo audiovisual trabajos que no hubiesen sido posibles sin los drones. Grabaciones en las que el riesgo humano y material que habr\'ia que asumir es tan alto que tan sólo son factibles de este modo. Debemos por tanto a los drones fotograf\'ias como la de la Figura 1.2 que si bien no se tom\'o como parte de ning\'un proyecto cinematogr\'afico nos hace una idea de lo que la rob\'otica a\'erea puede ofrecernos.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.45]{img/drone_volcan.jpg}
  \caption{Drone grabando en el interior de un cráter}
  \label{fig:drone_volcan}
\end{figure}
Otro sector que ha adaptado r\'apidamente el uso de \'estas pequeñas aeronaves es el agropecuario, donde se utiliza para medir la condiciones del terreno, con el fin de recoger informaci\'on sobre la hidrataci\'on, la temperatura o el ritmo de crecimiento de los cultivos. Con el punto de vista e im\'agenes de los drones el agricultor puede tener en tiempo real una foto del estado de su viñedo localizando con facilidad zonas afectadas por plagas o incluso vigilar el cultivo para evitar la entrada de intrusos. Controlan el riego e incluso esparcen los pesticidas de manera eficiente siendo un arma eficaz contra las plagas, se utilizan incluso como espantap\'ajaros.
La aplicaci\'on de drones en este sector se remonta a 1983 cuando el Ministerio de Agricultura de Jap\'on preocupado por el envejecimiento de su poblaci\'on rural encarg\'o a Yamaha el desarrollo de una aeronave no tripulada capaz de realizar varias tareas de las anteriormente descritas, a fin de atraer m\'as gente al medio rural. En 1990 se entregaron las primeras unidades del Yamaha RMAX y actualmente el 40\% de los arrozales japoneses cuentan con un drone sobrevol\'andolos. Como m\'etodo de riego, el drone tiene dos contenedores que pueden albergar una cantidad de 8 litros cada uno y en caso de utilizarse para esparcimiento de semillas o material granulado, posee otros dos contenedores que se pueden adaptar los cuales tienen una capacidad de 13 litros cada uno. Puede elevarse a una altura de 400 metros y dispone de un sistema de GPS que no sólo le permite mayor estabilidad durante el vuelo, sino que tambi\'en le proporciona al usuario la posibilidad de programar la ruta desde antes de ser lanzado. Tiene una autonom\'ia de una hora durante el vuelo.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.50]{img/Yamaha_crop_Sprayer_Drone_RMAX.jpg}
  \caption{Drone Yamaha RMAX fumigando}
  \label{fig:drone_yamaha}
\end{figure}

Los drones est\'an empezando a popularizarse en las empresas de seguridad privada. El punto de vista y alcance que proporcionan hacen de los drones herramientas muy potentes y vers\'atiles. Se utilizan principalmente en vigilancia perimetral, d\'onde un drone puede recorrer una ruta de forma aut\'onoma proporcionando im\'agenes de la misma a los vigilantes del centro de c\'amaras o a los \textit{smartphones} o \textit{tablets} de los guardias de seguridad. Se les utiliza tambi\'en como apoyo a la vigilancia tradicional, sobrevolando con ellos zonas de grandes aglomeraciones o apoyando desde las alturas a las patrullas de vigilancia proporcionando, por ejemplo, im\'agenes t\'ermicas de una zona durante la noche. 
Empresas como la francesa Drone Volt, l\'ider en el sector de los drones profesionales, ha realizado un avance en materia de seguridad con el modelo \textit{Z18 UF (Unlimited Flight)} un drone umbilical capaz monitorizar 24 horas de forma ininterrumpida. Esto es posible gracias a que va unido con un cable a una fuente de alimentaci\'on en tierra y por tanto se reduce la principal limitaci\'on de los drones, el tiempo de vuelo que proporcionan bater\'ias pequeñas y ligeras.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.25]{img/droneVoltZ18ufphoto.jpg}
  \caption{Drone Volt Z18 UF}
  \label{fig:drone_volt}
\end{figure}

En España varias empresas de vigilancia privada ofrecen sus servicios con drones, algunas de ellas son: 
\begin{itemize}
\item Aero C\'amaras - http://aerocamaras.es/servicios-drones-profesionales/drones-vigilancia
\item Prosegur - https://www.prosegur.es/newsdetails/drone-vigilancia-interiores
\item SkyDron - https://www.skydron.es/seguridad-aerea-privada-drones/
\end{itemize}

Son utilizados tambi\'en por los servicios de rescate tras una cat\'astrofe para evaluar los daños experimentados y ayudar a encontrar supervivientes entre los escombros. Algunos son capaces de enviar a los supervivientes paquetes de supervivencia con salvavidas, alimentos o agua mientras esperan su rescate\footnote{\url{http://www.elmundo.es/economia/2015/09/15/55f8239546163fc6598b45c3.html}}

Es muy extendido en el mantenimiento de infraestructuras, en trabajos como:
\begin{itemize}
\item La inspecci\'on de tendidos el\'ectricos. Donde antes se utilizaban helic\'opteros ahora con los drones se obtienen mejores im\'agenes e inspecciones más precisas y meticulosas sin arriesgar vidas humanas y por una m\'inima parte del coste que se asum\'ia. 
\item Estos mismos drones tambi\'en se utilizan para la revisi\'on de las palas de los aerogeneradores, evitando que el personal de mantenimiento tenga que descolgarse desde ellos para su an\'alisis.
\item Se utilizan tambi\'en drones para revisar oleoductos, especialmente en zonas de dif\'icil acceso.
\item Conservaci\'on de carreteras, detectando el estado de conservaci\'on de las calzadas, obst\'aculos o potenciales situaciones de peligro debido a accidentes o condiciones meteorol\'ogicas adversas.
\item Supervisi\'on y mantenimiento de presas y embalses.
\end{itemize}
La empresa española Ferrovial fue la pionera en España en obtener la licencia para este tipo de trabajos\footnote{http://www.ferrovial.com/es/prensa/noticias/ferrovial-servicios-obtiene-la-licencia-para-operar-drones-en-espana/}

Es muy importante su uso en topograf\'ia, principalmente para obtener topograf\'ia a\'erea mediante t\'ecnicas de fotogrametr\'ia\footnote{T\'ecnica para obtener mapas y planos de grandes extensiones de terreno por medio de la fotograf\'ia a\'erea.}. La empresa española OHL dispone de varios drones a tal efecto y tiene varios proyectos de I+D+I para seguir profundizando en esta materia. 
De esta manera se pueden estudiar obras en su fase de licitaci\'on, realizar c\'alculos de vol\'umenes y superficies en acopios, control de certificaciones, estudio de patolog\'ias como deslizamiento de taludes y realizar seguimientos.
 
Se utilizan en minas a cielo abierto para entre otras muchas funciones:
\begin{itemize}
\item Control y monitorizaci\'on de explotaci\'on de minerales y su impacto ambiental
\item Observaci\'on de operaciones con necesidad de supervisi\'on a\'erea y seguimiento de movimientos de tierra, residuos, balsas, control de relaves,control de pilas,transporte de implementos de un punto a otro.
\item Realizar la reconstrucci\'on de una mina mediante fotogrametr\'ia para medir los vol\'umenes extra\'idos.
\item Analizar la variaci\'on de alturas entre periodos para determinar los vol\'umenes explotados por periodo.
\end{itemize}
E incluso se han empezado a utilizar por el Ministerio de Hacienda de España con fines recaudatorios. Sobrevuelan las viviendas o fincas localizando edificaciones no registradas por el catastro con el fin de regularizar su situaci\'on.

El uso de drones es especialmente importante en el \'ambito cient\'ifico para, por ejemplo, tomar medidas de temperatura y CO2 en zonas peligrosas\footnote{http://www.igepn.edu.ec/servicios/noticias/1395-medidas-de-temperatura-y-co2-de-las-fumarolas-muestreo-y-fotografias-con-drone}, estudiar las nubes volc\'anicas\footnote{https://www.nasa.gov/topics/earth/earthmonth/volcanic-plume-uavs.html} o volar dentro de una grieta en un glaciar\footnote{http://tn.com.ar/tecno/recomendados/increible-un-drone-volo-dentro-de-un-glaciar\_648661}. 

El futuro del uso de los drones se est\'a escribiendo en este mismo momento y es que poco a poco se investigan con nuevos usos o c\'omo mejorar los ya actuales. Uno de los usos m\'as prometedores que est\'a siendo investigado es el de log\'istica y paqueter\'ia. Amazon estudi\'o en 2105 la viabilidad de utilizar drones para el reparto, especialmente en zonas de dif\'icil acceso o bien alejadas de las zonas habituales de reparto. A finales de dicho año realiz\'o pruebas de entregas en el Reino Unido y desarroll\'o un prototipo de alrededor de 25Kg de peso. El objetivo de Amazon en \'este experimento era realizar entregas de hasta 3Kg de peso en menos de 30 minutos y ver la viabilidad de la entrega de paquetes en zonas pobladas.

Esto ha dado pie a las principales empresas de log\'istica y paqueter\'ia a realizar sus propios desarrollos y construir sus propios prototipos. 
DHL ha realizado sendas pruebas con drones de cerca de 5 Kg para la entrega de paquetes de hasta 1,2 Kg.
La empresa francesa GeoPost ha comprado recientemente la empresa Atechsys, especializada en el desarrollo de sistemas aut\'onomos para aeronaves no tripuladas. Operaci\'on que ha dado como resultado un drone que cuenta con seis rotores el\'ectricos y estructura de fibra de carbono, con capacidad para llevar paquetes de un peso aproximado de hasta 2 kilos.
UPS ha realizado un experimento en Tampa con un octoc\'optero que despegar\'ia desde el techo de la furgoneta de reparto para entregar los paquetes en zonas rurales y ahorrar en kilometraje el drone tend\'ia una carga de pago de unos 4,5Kg.

Otro de los estudios que m\'as llama la atenci\'on es el de Google y Facebook que compiten en llevar internet a zonas aisladas a trav\'es de una red de drones y sat\'elites.
Facebook present\'o en su F8 en Marzo de 2015 su prototipo de drone a tal efecto, Aquila, fruto de la adquisici\'on de la empresa especializa en rob\'otica a\'erea Acenta. Este proyecto se enmarca dentro del plan internet.org y está liderado por Connectivity Lab\footnote{Un equipo formado por 50 expertos en aeron\'autica y ciencia espacial} que pretende ofrecer internet con un coste reducido a todo aquel que no lo tenga.

En 2014 Google por su parte compr\'o Titan Aeroespace para crear una flota de drones propulsados por energ\'ia solar, capaces de volar m\'as de una semana mientras tomaban fotos de la superficie y prove\'ian de acceso a Internet a lugares remotos y aportar nueva informaci\'on para sus mapas. Este desarrollo sin embargo se ha abandonado en enero de este año en pro de la utlizacion de globos aerost\'aticos para tal prop\'osito.
 
\section{Rob\'otica a\'erea en el laboratorio de Rob\'otica de la URJC}
\label{sec:lab_robotica}

Dentro del laboratorio de rob\'otica de la Universidad Rey Juan Carlos cabe destacar varios trabajos realizados para profundizar, investigar y experimentar con drones y que han servido de antecedentes directos y base para este Proyecto Fin de Carrera. Todos ellos han usado como plataforma software JdeRobot \url{http://jderobot.org/}. Se resumen a continuaci\'on los m\'as importantes.

Trabajos como el de Alberto Mart\'in\cite{amartinflorido:_mediawiki} que permiti\'o controlar un AR-Drone 2 de Parrot real desde una aplicaci\'on cliente, desarrollando para ello un driver para el dron, \texttt{ardrone\_server} y una aplicacci\'on cliente \texttt{UAV Viewer}.
El driver \texttt{ardrone\_server} es capaz de conectar con el AR-DRONE a trav\'es de comandos AT tanto para obtener los datos de todos sus sensores como para controlarlo y expone todo ello en interfaces JdeRobot para su interconexi\'on con el software del entorno.

El AR-Drone es un quadric\'optero que en su versi\'on actual cuenta con aceler\'ometros, gir\'oscopos y magnet\'ometros en 3 ejes que determinan junto con su bar\'ometro y un sensor de ultrasonidos la actitud del mismo, dispone adem\'as de 2 c\'amaras, una ventral y otra frontal, y algunos modelos traen sensor GPS.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.16]{img/ar-drone2.jpg}
  \caption{AR Drone 2}
  \label{fig:ardrone2}
\end{figure}

Alberto desarroll\'o tambi\'en el software de control \texttt{UAV Viewer}. Esta aplicaci\'on cliente es capaz de controlar con cualquier drone y mostrar de forma visual su actitud, lo que captan sus c\'amaras.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.30]{img/uavviewer.png}
  \caption{Ventanas de UAV Viewer}
  \label{fig:uavviewer}
\end{figure}

Daniel Yague\cite{daniyague:_mediawiki} desarroll\'o un driver (plugin) para emular el comportamiento del AD-Drone en el simulador Gazebo, de referencia en el Laboratorio de Rob\'otica de la universidad y contenido dentro de la suite JdeRobot. Con este driver se hizo posible probar, anticipar los problemas que puedan surgir en el vuelo del drone antes siquiera de volarlo f\'isicamente. De esta forma es posible el desarrollo de aplicaciones de navegaci\'on complejas sin disponer de \'el, sin arriesgarlo e independientemente de factores externos como la climatolog\'ia.
Para mostrar las posibilidades que se abr\'ian desarroll\'o varias aplicaciones en el ar-drone simulado como un gato-rat\'on donde un drone trataba de perseguir autónomommanete a otro que se teleloperaba o una aplicaci\'on donde el drone segu\'ia una carretera.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.57]{img/ArDrone_model.png}
  \caption{AR Drone Simulado en Gazebo}
  \label{fig:ardrone_gazebo}
\end{figure}

Jorge Cano construy\'o físicamente su propio drone utilizando como base un quadric\'optero con un procesador Intel Compute Stick (ICS) STCK1A8LFC como ordenador de abordo y una placa Pixhawk como estabilizadora/piloto autom\'atico\cite{jocano:_mediawiki}. Esta placa utiliza como protocolo de comunicaci\'on MAVLink\footnote{Micro Air Vehicle Link \url{https://en.wikipedia.org/wiki/MAVLink}} y dispone de aceler\'ometros, gir\'oscopos y magnet\'ometros para determinar la actitud.
Adem\'as de la construcci\'on del drone Jorge, desarroll\'o el driver MAVLinkServer donde apoy\'andose en MAVProxy adapt\'o los comandos MAVLink a interfaces JdeRobot permitiendo acceder a la actitud y controlar el drone con comandos tipo GotoXY enviados a trav\'es del interfaz Pose3D\footnote{Ver cap\'itulo 3 Arquitectura utilizada}

\begin{figure}[h]
\centering
  \subfloat[Intel]{
   \label{f:Ordenador de abordo}
    \includegraphics[width=0.3\textwidth]{img/Compuandcaminstall.JPG}}
  \subfloat[camaras]{
   \label{f:C\'amaras de abordo}
    \includegraphics[width=0.535\textwidth]{img/FPVcamara.jpg}}
 \label{f:drone_cano}
\end{figure}

En la actualidad se est\'an desarrollando varios trabajos sobre este tipo de placas estabilizadoras que tiene como software base ArduCopter/Ardupilot o son compatibles. Por un lado Diego Jim\'enez\cite{jimenez:_mediawiki} trabaja en controlar un Solo Drone de la empresa 3DR\footnote{Empresa norteamericana con sede en California especializada en rob\'otica a\'erea. Se sit\'ua en 2017 como la 3ª empresa del sector} mediante el interfaz de velocidades \texttt{CMDVel}\footnote{Ver cap\'itulo 3 Arquitectura utilizada}. Este quadrac\'otero tiene como placa de control una Pixhawk como la utilizada por Jorge Cano en el drone que se construy\'o y utiliza tambi\'en comamdos MAVLink.

Por otro lado, Jorge Vela se encuentra desarrollando c\'omo realizar la maniobra de aterrizaje de forma autom\'atica al localizar un patr\'on o baliza mediante visi\'on\cite{jvela:_mediawiki}. Como drone de referencia est\'a utilizando un Solo Drone con un procesador Intel Stick como ordenador de abordo.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.60]{img/3DR-solo.jpg}
  \caption{SOLO Drone de 3D Robotics}
  \label{fig:3dr_solo}
\end{figure}

Crear clientes web para las principales aplicaciones JdeRobot, incluyendo una con drones, fue tarea de Aitor Mart\'inez\cite{aitor:_mediawiki}. En desarroll\'o seis clientes entre los que destaca la creaci\'on del cliente web similar a la herramienta UavViewer para teleoperar drones tanto reales como simulados y ver los datos de sus sensores. También la creacion de \texttt{IntrorobUavJS} que permite programar comportamientos aut\'onomos en JavaScript una en una interfaz Web similar a UAV Viewer. Gracias a estos desarrollos, se pod\'ia teleoperar un drone desde cualquier dispositivo que tenga un navegador web, como un m\'ovil o una tablet.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.26]{img/js-jderobot1.png}
  \caption{Drone teleoperado con tel\'efono m\'ovil}
  \label{fig:js_jderobot1}
\end{figure}

En este contexto este PFC aborda el soporte de un nuevo robot \'aereo, diferente a todos los usados en el Laboratorio de Rob\'otica hasta la fecha: un avi\'on de ala fija. Adem\'as se integrar\'a con las herramientas ya existentes para robots a\'ereos dentro de la plataforma JdeRobot.

La memoria de este PFC se ha vertebrado en siete cap\'itulos. En el segundo se fijan los objetivos concretos que se persiguen y los requisitos deseables. A continuaci\'on se describe la infraestructura, tanto hardware como software, utilizada en el desarrollo de este trabajo. En el cap\'itulo 4 se detalla el desarrollo del driver \texttt{APM Server} y se desgrana la aplicaci\'on \texttt{UAV Commander} en el cap\'itulo 5. En el sexto se muestran los experimentos realizados con todo el software desarrollado y en el s\'eptimo las conclusiones alcanzadas.

\chapter{Objetivos}
\label{sec:objetivos}


\section{Problema abordado}
\label{problema}
El objetivo general de este proyecto final de carrera es dar soporte en la plataforma software JdeRobot a drones de ala fija que utilicen el protocolo de comunicaci\'on MAVLink. Este objetivo general lo hemos articulado en 3 subobjetivos:
\begin{enumerate}
\item Desarrollo de un driver que acceda a los sensores y actuadores de drones de ala fija que utilicen \texttt{MAVLink}, y dar soporte a la actuaci\'on de misiones, que hasta ahora no soportaba JdeRobot.
\item Desarrollo de una aplicaci\'on GCS \textit{Ground Control Station} que permita al operador humano introducir misiones y seguir el cumplimiento de las mismas a trav\'es de ella. Permitiendo el acceso a la actitud y a las c\'amaras de abordo.
\item Hacer experimentos con aviones MAVLink tanto simulados como con un prototipo real. Conectaremos nuestro driver al simulador SITL y la aplicación al servidor para verificar la correcta especificación y materialización de las misiones. Construiremos un prototipo de avión \texttt{MAVLink} y comprobaremos la conexión con el driver y la recepción de toda la información sensorial.

\end{enumerate}

Tanto el driver como la aplicación, han de ser multiplataforma, utilizar \'unicamente librer\'ias de software libre y ser 100\% compatibles con los actuales interfaces JdeRobot

\section{Metodolog\'ia}
\label{metodologia}

Este proyecto se ha abordado con dos metodolog\'ias de trabajo distintas combinadas, la primera es el desarrollo en espiral que se ha utilizado como metodolog\'ia principal, y la segunda la metodolog\'ia \'agil Kanban, para un control de tareas y subtareas m\'as eficiente.

\begin{figure}[h]
\centering
  \subfloat[Representaci\'on gr\'afica del desarrollo en espiral.]{
   \label{f:espiral}
    \includegraphics[width=0.4\textwidth]{img/359px-ModeloEspiral.png}}
  \subfloat[Representaci\'on de una pizarra kanban]{
   \label{f:kanban}
    \includegraphics[width=0.52\textwidth]{img/kanban.png}}
    \caption~{Metodologías empleadas}
\end{figure}


El modelo de desarrollo en espiral define una serie de ciclos que se repiten en bucle hasta el final del proyecto, dividi\'endolo
en varias subtareas m\'as sencillas y estableciendo puntos de control al final de cada iteraci\'on, en los que se
eval\'ua el trabajo realizado y se enfocan las nuevas tareas para continuar.
Esta metodolog\'ia recibe su nombre por la forma de espiral que tiene su representaci\'on gr\'afica o diagrama
de flujo, que podemos ver en la figura 2.1. En cada iteraci\'on se llevan a cabo las siguientes actividades:
\begin{enumerate}
	\item Determinar los objetivos, dividir en subobjetivos y fijar requisitos.
	\item Analizar los riesgos y factores que impidan o dificulten el trabajo y las consecuencias negativas que éste pueda ocasionar.
	\item Desarrollar los objetivos cubriendo los requisitos analizados anteriormente.
	\item Se analizan los resultados del desarrollo y las pruebas y en base a esto se decide el inicio del siguiente ciclo en una reunión con el tutor.
\end{enumerate}



Durante el ciclo de vida del proyecto se han llevado a cabo reuniones semanales de seguimiento con el tutor. En ellas
se evaluaban las tareas realizadas y se marcaba qu\'e direcci\'on tomar para la siguiente iteraci\'on o incremento. Si los
puntos marcados en la anterior reuni\'on no se hab\'ian alcanzado se ampliaba el plazo o se discut\'ian otras
v\'ias para avanzar. En caso contrario se propon\'ian nuevos subobjetivos.


Para desarrollar las tareas especificadas en cada incremento, fijadas en cada reuni\'on de seguimiento, se utiliz\'o la metodolog\'ia kanban. Esta metodolog\'ia de desarrollo no es m\'as que una adaptaci\'on de su versi\'on industrial que surgi\'o en Toyota. A finales de los años 40, Toyota empez\'o a optimizar sus procesos de ingenier\'ia a partir del modelo que empleaban los supermercados para llenar los estantes. Los supermercados almacenan los productos suficientes para suplir la demanda del cliente, una pr\'actica que optimiza el flujo entre el supermercado y el cliente. 
En su versi\'on TIC \'esta metodolog\'ia se centra, al igual que su versi\'on industrial, en el \textit{just in time}\footnote{ El JIT es una política de mantenimiento de inventarios al mínimo nivel posible donde los suministradores entregan justo lo necesario en el momento necesario para completar el proceso productivo.} y permite apreciar de una forma muy visual las tareas que hay en vuelo, desarrolladas, pendientes o bloqueadas pudiendo anticiparnos a cuellos de botella o bloqueos de forma sencilla.

\begin{figure}[h]
\centering
   \label{f:kanban}
    \includegraphics[width=0.31\textwidth]{img/IMG_0859.jpg}
   \label{f:espiral}
    \includegraphics[width=0.31\textwidth]{img/IMG_0864.jpg}
   \label{f:kanban}
    \includegraphics[width=0.31\textwidth]{img/IMG_0865.jpg}
   \label{f:kanban}
    \includegraphics[width=0.31\textwidth]{img/IMG_1016.jpg}
   \label{f:espiral}
    \includegraphics[width=0.31\textwidth]{img/IMG_1029.jpg}
   \label{f:kanban}
    \includegraphics[width=0.31\textwidth]{img/IMG_1037.jpg}
   \label{f:kanban}
    \includegraphics[width=0.31\textwidth]{img/IMG_1043.jpg}
   \label{f:espiral}
    \includegraphics[width=0.31\textwidth]{img/IMG_1249.jpg}
   \label{f:kanban}
    \includegraphics[width=0.31\textwidth]{img/IMG_0063.jpg}
\caption{Evoluci\'on de nuestra pizarra Kanban}
\end{figure}


Para apoyarnos en nuestro desarrollo hemos utilizado cuatro herramientas:
\begin{itemize} 
\item GitHub como forja y control de versiones. En el repositorio \url{https://github.com/RoboticsURJC-students/2014-pfc-JoseAntonio-Fernandez}  se almacenan todos los desarrollos de este PFC. Tambi\'en se encuentran subproductos de desarrollo que han ido surgiendo como apoyo o pruebas a los desarrollos principales.
\item Bitácora web en JdeRobot donde hemos actualizado peri\'odicamente nuestros avances acompañados con explicaciones, v\'ideos e im\'agenes. Su url es \url{http://jderobot.org/Jafernandez} y aqu\'i se puede contemplar con m\'as detalle la construcci\'on del UAV.
\item YouTube y ftp. como herramientas de almacenamiento de los vídeos de nuestra bitácora web y de nuestros experimentos.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INFRAESTRUCTURA %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Infraestructura utilizada}
\label{chap:infraestructura_utilizada}

Se describen en este capítulo las herramientas, ingredientes hardware y software en las que se apoya este PFC.

\section{Hardware}
\label{sec:hardware}

Este PFC utiliza principalmente tres componentes hardware: un avión de radio control, un procesador Raspberry PI 3 y una placa estabilizadora.


Como el avi\'on de radio-control hemos elegido el avi\'on Bix3, distribuido por la empresa china \url{www.hobbyking.com}. Hemos elegido este modelo por ser un avi\'on muy estable debido principalmente a sus cualidades como velero y su alta superficie alar. Otro aspecto importante de la construcci\'on del avi\'on es que el propulsor no se encuentra en el frontal del avi\'on lo que nos permitir\'a aprovechar al m\'aximo esa zona pudiendo poner incluso una c\'amara frontal.

\begin{figure}[h]
\centering
  \subfloat[Bix3]{
   \label{f:Bix3}
    \includegraphics[width=0.339\textwidth]{img/bix_general.jpg}}
  \subfloat[Vista de cerca]{
   \label{f:Vista de cerca}
    \includegraphics[width=0.339\textwidth]{img/bix_prof.jpg}}
  \subfloat[Bix3 en vuelo]{
   \label{f:Bix en vuelo}
    \includegraphics[width=0.3\textwidth]{img/bix_vuelo.png}}
 \label{f:drone_cano}
\end{figure}

Para poder cargar con el equipo necesario a bordo ha sido necesario cambiar el motor, el variador y las bater\'ias de serie por otras de mayor rendimiento que nos permitan volar con tanta carga de pago.

Una raspberry PI3 es el ordenador de abordo y en el que se ha instalado el software necesario para dotarle de inteligencia. Hemos elegido este dispositivo debido al compromiso peso/potencia que otorga, as\'i como porque es un hardware muy asequible  y extendido. Conectado a esta PI3 va una PiCam que nos permitirá ver lo que el avi\'on vea.

Para este PFC hemos utlizado una placa Ardupilot Mega\footnote{\url{http://www.ardupilot.co.uk/}} como estabilizador/piloto automático. Este dispositivo tiene como base una placa Arduino Mega a la que se le han incorporado gir\'oscopos y acerel\'ometros en 3 ejes para su estabilizaci\'on y que trae de serie un receptor GPS con br\'ujula. El kit que adquirimos incluye tambi\'en todo lo necesario para integrarlo en nuestro avión y proporcionarle la alimentación necesaria. 
Esta placa actúa directamente sobre los los servos y el motor del avión a trav\'es de señales PWM. 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.50]{img/ardupilot.jpg}
  \caption{Placa estabilizadora ardupilot}
  \label{fig:ardupilot}
\end{figure}

Esta placa ha de \textit{flashearse} con un \textit{firmware} en funci\'on del tipo de aeronave que utilicemos como base, Arducopter para alas rotatorias o Ardupilot para alas fijas. Ésta y otras placas similares como PixHawk, se comunican y aceptan comandos a través de un protocolo llamado MAVLink. A trav\'es de comandos MAVLink se puede acceder a los sensores, tanto los que incorpora la placa nativa cómo a los que le añadamos a la misma, como el GPS o sensores de velocidad del aire. A trav\'es de estos comandos se le puede tambi\'en enviar órdenes al piloto autom\'atico, quien las ejecutar\'a. M\'as adelante trataremos el protocolo MAVLink en profundidad. Cabe destacar que algunos de los comandos que mandemos tendrán distinto comportamiento en la placa en función del \textit{firmware} escogido, y que hay un conjunto de comandos que no pueden ser utilizados en uno o en otro.
El objetivo de este PFC es crear un driver que nos permita comunicarnos con esta placa y una aplicación en JdeRobot para el manejo de un avión robotizado.


\section{JdeRobot}
\label{sec:jderobot}

JdeRobot es un entorno de software creado por el laboratorio de rob\'otica de la Universidad Rey Juan Carlos, para el desarrollo de aplicaciones de rob\'otica. Su \'ultima versión, la 5.5, se liber\'o el 15 de Marzo de 2017 pudiendo ver los detalles de \'esta en el github oficial\footnote{\url{https://github.com/JdeRobot/JdeRobot/wiki/JdeRobot-5.5.0}}.
JdeRobot se compone de interfaces, drivers, utilidades y aplicaciones para el desarrollo de cualquier proyecto de rob\'otica. Gracias a estos interfaces y al \textit{middleware} ICE (de la empresa ZeroC) se interconectan entre s\'i todos los componentes de una aplicación robótica.
Algunos de los drivers m\'as importantes que contiene, y más relacionados con este trabajo, son:
\begin{enumerate}
\item \texttt{Cameraserver}. Un driver para enviar im\'agenes y video a trav\'es del interfaz \texttt{camera}
\item \texttt{Gazeboserver}. Conjunto de \textit{plugins} de Gazebo para conectar las aplicaciones con distintos robots simulados (con ruedas, cuadricópteros, etc).
\item \texttt{MAVLinkServer}. Driver desarrollado para intercomunicar JdeRobot con placas de estabilización y autopiloto que utilicen el protocolo de comunicaci\'on MAVLink.
\item \texttt{Ardrone\_server}. Driver que conecta el Parrot Ar-Drone a JdeRobot. Este driver escrito en c++ transforma el conjunto de comandos AT del drone en interfaces ICE y viceversa. Implementa los interfaces \texttt{camera}, \texttt{cmdvel}, \texttt{navdata}, \texttt{extra} y \texttt{pose3D} y permite acceder a la actitud del drone as\'i como a sus 2 c\'amaras. Sirve tambi\'en datos como el nivel de la bater\'ia y permite grabar v\'ideo o tomar fotos.
\end{enumerate}
Algunas de las herramientas contenidas en JdeRobot y más relacionadas con este PFC son:
\begin{enumerate}
\item \texttt{Cameraview}. Es una aplicaci\'on desarrollada en c++ capaz de recibir v\'ideo a trav\'es del interfaz \texttt{camera}.
\item \texttt{UAV viewer}. Aplicaci\'on desarrollada para teleoperar robots a\'ereos. Esta aplicaci\'on permite teleoperar cualquier tipo de robot a\'ereo y ofrece de forma visualmente atractiva datos como la actitud, velocidades lineales y angulares, ofrece tambi\'en la posibilidad de visualizar imágenes servidas por el interfaz \texttt{camera}.
\end{enumerate}
\subsection{Interfaces}

JdeRobot expone m\'as de 30 interfaces pero en este cap\'itulo explicaremos los que durante nuestro desarrollo hemos implementado en el driver para el avión o usado en la aplicación:
\begin{itemize}
\item \texttt{Pose3D}. Utilizado para recoger los datos de actitud y la posici\'on 3D de la aeronave. Devuelve la posición del robot y su orientacion en el espacio, empleando para expresar la orientación cuaterniones. Comparados con los ángulos de Euler, son más simples de componer y evitan el problema del bloqueo del cardán\footnote{El bloqueo del cardán consiste en la pérdida de un grado de libertad en una suspensión cardán de tres rotores, que ocurre cuando los ejes de dos de los tres rotores se colocan en paralelo, bloqueando el sistema en una rotación en un espacio bidimensional degenerado.}, \texttt{pymavlink} nos permite transformar de uno a otro de forma sencilla.
{\scriptsize
\begin{verbatim}
Pose3DData
  {
	float x;  //latitude
	float y;  //longitude
	float z;  //altitude
	float h;  //not used now
	float q0; //quaternion component 1
	float q1; //quaternion component 2
	float q2; //quaternion component 3
	float q3; //quaternion component 4
  };
\end{verbatim}}
\item \texttt{Camera}. Utilizado para servir im\'agenes.
{\scriptsize
\begin{verbatim}
  class CameraDescription
  {
    string name;
    string shortDescription;
    string streamingUri;
    float fdistx; //focal distance in x
    float fdisty; //focal distance in y
    float u0;
    float v0;
    float skew;
    float posx; //position in x of the camera
    float posy; //position in y of the camera
    float posz; //position in z of the camera
    float foax; //position of a focused object
    float foay; //position of a focused object
    float foaz; //position of a focused object
    float roll; //roll of the camera
  };

\end{verbatim}}
Los parámetros del interfaz se pueden dividir en 2 bloques: intrínsecos y extrínsecos. 
\begin{itemize}
\item Intrínsecos: fdistx, fdisty, u0, v0 y skew.
\item Extrínsecos: posx, posy, posz, foax, foay, foaz y roll.
\end{itemize}
\item \texttt{NavData}. Utilizado para servir datos secundarios de actuaci\'on como velocidades lineales o angulares o el estado de la bater\'ia.
{\scriptsize
\begin{verbatim}
class NavdataData 
{
 int vehicle; //0-> ArDrone1, 1-> ArDrone2
 int state; // landed, flying,...
 float batteryPercent; //The remaing charge of baterry %
		
 //Magnetometer Ardron2.0
 int magX;
 int magY;
 int magZ;
		
 int pressure; //Barometer Ardron2.0
 int temp;     //Temperature sensor Ardron2.0
 float windSpeed; //Estimated wind speed Ardron2.0		
		
 float windAngle;
 float windCompAngle;
 
 float rotX; //rotation about the X axis
 float rotY; //rotation about the Y axis		
 float rotZ; //rotation about the Z axis
 
 int altd; //Estimated altitude (mm) 

 //linear velocities (mm/sec)
 float vx;
 float vy;
 float vz;
		
 //linear accelerations (unit: g) ¿Ardron2.0?
 float ax;
 float ay;
 float az;

 //Tags in Vision Detectoion
 //Should be unsigned
 int tagsCount;
 arrayInt tagsType;
 arrayInt tagsXc;
 arrayInt tagsYc;
 arrayInt tagsWidth;
 arrayInt tagsHeight;
 arrayFloat tagsOrientation;
 arrayFloat tagsDistance;

 float tm; //time stamp
};
\end{verbatim}}
\item \texttt{Extra}. Utilizado principalmente para las \'ordenes de despegue y aterrizaje.
{\scriptsize
\begin{verbatim}
    void land() - land drone. 
    void takeoff() - takeoff drone. 
    void reset() 
    void recordOnUsb(bool record) 
    void ledAnimation(int type,float duration, float req) 
    void flightAnimation(int type, float duration) 
    void flatTrim() 
    void toggleCam() - switch camera. 
\end{verbatim}}

\end{itemize}


\section{Protocolo \texttt{MAVLink}}
\label{sec:mavlink}

MAVLink son siglas de Micro Air Vehicle Link, un protocolo de comunicaci\'on desarrollado para comunicar las placas estabilizadoras dotadas de piloto autom\'atico con los GCS o \textit{Ground control station}, es decir, con las aplicaciones desde las que se pod\'ia enviar misiones y seguir el cumplimiento de las mismas desde tierra.
MAVLink se public\'o en 2009 por Lorenz Meier, publicado bajo licencia LGPL. Aspira a convertirse en el protocolo standard en rob\'otica a\'erea y se ha probado su funcionamiento en PX4, PIXHAWK, APM\footnote{Ardupilot Mega} y Parrot AR.Drone.

Un ejemplo de comando MAVLink ser\'ia este mensaje que porta la informaci\'on del GPS y se env\'ia peri\'odicamente en ciclos que decidimos según la configuración de conexi\'on con el dispositivo.
{\scriptsize
\begin{verbatim}
type GpsStatus struct {
    SatellitesVisible  uint8      Número de satélites visibles
    SatellitePrn       [20]uint8  Id Global de cada satélite
    SatelliteUsed      [20]uint8  Lista con el uso de cada sat\'elite
    SatelliteElevation [20]uint8  Elevación, nos da el ángulo sobre el horizonte.
    SatelliteAzimuth   [20]uint8  Dirección del satélite, 0: 0 grados, 255: 360 grados.
    SatelliteSnr       [20]uint8  Señal/ruido de cada uno de los satélites
}
\end{verbatim}}

Otro mensaje, esta vez vinculado a la actuaci\'on es:
{\scriptsize
\begin{verbatim}
type MissionItem struct {
    Param1          float32    parámetro variable en función del comando.
    Param2          float32    parámetro variable en función del comando.
    Param3          float32    parámetro variable en función del comando.
    Param4          float32    parámetro variable en función del comando.
    X               float32    latitud
    Y               float32    longitud
    Z               float32    altitud
    Seq             uint16     Número del item en la misión
    Command         uint16     Tipo de comando de navegación.
    TargetSystem    uint8      ID del sistema
    TargetComponent uint8      
    Frame           uint8      Sistema de coordenadas que se utiliza.
    Current         uint8      Misión actual no:0, si:1
    Autocontinue    uint8      Autocontinuar al siguiente objeto de misión.
}
\end{verbatim}}
Este mensaje representa in hito de misión, una misión se compone de uno o varios de estos hitos. Un hito de misión en MAVLink puede ser un punto de paso (o \textit{waypoint}), una maniobra de despegue o aterrizaje, o instrucciones de cambio de altura o velocidad del robot. 

\section{Biblioteca pymavlink}
\label{sec:mavlink}

Pymavlink trae la definici\'on de los comandos MAVLink a utilizar desde un programa escrito en python. Gracias a esta biblioteca no se tienen que construir los comandos MAVLink a mano, evitando cometer errores y simplificando la creación y envío de los mismos. Su uso facitita también la identificación de los comandos recibidos. Para instalarla tan s\'olo se necesita ejecutar:
 
\begin{verbatim}
sudo pip2 install -U pymavlink
\end{verbatim}

Y tiene como dependencias:
\begin{itemize}
\item future. Se necesita future como soporte en Python 3.X de Python 2.7 (http://python-future.org/)
\item lxml. Para analizar xml (http://lxml.de/installation.html)
\item python-dev
\item MavLink. (http://qgroundcontrol.org/mavlink/start)
\end{itemize}


\section{Lenguaje Python y biblioteca PyQt5}
\label{sec:python}

Python es un lenguaje de programaci\'on interpretado y multiplataforma que naci\'o en los años 80 en los pa\'is bajos con idea de hacer m\'as legible el c\'odigo.
El lenguaje de programaci\'on que inicialmente se utilizaba principalmente para scripting, ha sabido crecer con los años y con la publicaci\'on de Python3 en 2009 ha recibido el impulso que necesitaba para ser hoy en d\'ia el quinto lenguaje m\'as utilizado, por encima de PHP, .NET y JavaScript (que baja hasta el 8º puesto seg\'un TIOBE\footnote{\url{https://www.tiobe.com/tiobe-index/}} en un estudio de Abril de 2017).

Para este PFC se eligió Python, porque mantiene el car\'acter multiplataforma de JdeRobot, su c\'odigo es simple y legible y trabaja muy bien con dependencias muy utilizadas en rob\'otica como OpenCV.

Para nuestro desarrollo hemos utilizado PyQt5 para desarrollar el interfaz gr\'afico. PyQt5 es un \textit{binding} de Qt5 en forma de librer\'ia Python que nos permite acceder a toda la funcionalidad de Qt5. Qt, propiedad de Nokia, es un conjunto de librer\'ias escritas en C++ para interfaces gr\'aficas.


\section{Mapas y Geo-referenciaci\'on}
\label{sec:mapas}

En la aplicación de navegación autónoma hemos tenido que trabajar con mapas geo-re\-fe\-ren\-cia\-dos. Estos mapas, que tanto se han popularizado gracias a Google, son una herramienta imprescindible en rob\'otica a\'erea si se quiere trabajar con largas distancias. Un mapa geo-referenciado es aquel en el que conocemos o podemos calcular la posici\'on en el planeta que representa cada píxel del mismo.

La forma m\'as com\'un de obtener estos mapas geo-referenciados es a trav\'es de WMS, siglas de \textit{Web Map Service}. Un WMS nos es m\'as que un servicio web que recibe como entrada unas coordenadas y una serie de par\'ametros y devuelve una imagen encuadrada en los datos enviados.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.501]{img/imageWithDisclaimer.png}
  \caption{Imágen geo-referenciada del club de aeromodelismo Icaro}
  \label{fig:sitl}
\end{figure}

En nuestro desarrollo hemos utilizado dos WMS: el del Instrituto Geogr\'afico Nacional de España (IGN) y el de Google.
A continuaci\'on vamos a desgranar un WMS, el de PNOA del IGN. PNOA son las siglas de Plan Nacional de Ortofotografía Aérea y contiene los mapas más actuales del territorio español. El WMS de PNOA requiere como entrada cuatro atributos principales:
\begin{enumerate}
\item La posici\'on GPS.
\item \textit{Bounding box}. El \textit{bounding box} son dos puntos que corresponden con la posici\'on GPS que queremos que sea el extremo inferior izquierdo de nuestra imagen y con el punto superior derecho de la misma.
\item Datum. El datum es el sistema de referencia o proyecci\'on de la tierra a utilizar. En el caso de IGN pese a que soporta varias utilizamos WGS84 que es el estándar.
\item Tamaño de la imagen, el tamaño que queremos que tenga la imagen obtenida.
\end{enumerate}
El resultado de la consulta es un mapa del que conocemos el punto central, y los extremos inferior izquierdo y superior derecho y el número de píxeles que tiene a lo ancho y alto del mismo, es decir, un mapa geo-referenciado.

Con el fin de facilitar el montaje y envío de las peticiones al WMS de PNOA utlizamos la librería OWSLib\footnote{\url{https://geopython.github.io/OWSLib/}} en su versión 0.14.0.


\section{Simulador SITL}
\label{sec:SITL}

\textit{Software In The Loop} (o SITL\footnote{\url{http://ardupilot.org/dev/docs/sitl-simulator-software-in-the-loop.html}}) es un simulador que permite a cualquier programa que env\'ie o reciba comandos MAVLink ejecutar pruebas sin necesidad de tener ninguna placa estabilizadora real y evitando la p\'erdida de la aeronave en caso de error del programa.
SITL se conecta con JSBSim, un simulador de vuelo de software libre para ejecutar el aspecto f\'isico de la simulaci\'on y con MAVProxy para el env\'io de comandos y seguimiento de misiones.
Se trata de una compilaci\'on en C++ de ardupilot y se podr\'ia asemejar al driver para cuadricópteros existente en JdeRobot \texttt{gazeboserver}, pero para aviones de ala fija.
SITL puede conectarse tambi\'en con Gazebo o con FlightGear para hacer m\'as completa su simulaci\'on mostrando el vuelo en un entorno desarrollado para pruebas con elementos del mundo real.
\begin{figure}
  \centering
  \includegraphics[scale=0.50]{img/SITL.jpg}
  \caption{Arquitectura de SITL}
  \label{fig:sitl}
\end{figure}

\cleardoublepage
\chapter{Driver APM Server}

El driver \texttt{APM Server} es quien va a mediar entre las aplicaciones de JdeRobot y el robot a\'ereo con sus sensores y actuadores f\'isicos o simulados a trav\'es de SITL. De esta manera las aplicaciones pueden correr en m\'aquinas distintas, no obligatoriamente a bordo, y pueden estar implementadas en distintos lenguajes de programaci\'on. Estas ventajas vienen de utilizar la divisi\'on habitual en JdeRobot entre componentes drivers y componentes aplicaci\'on, que se pueden conectar entre sí remotamente.

En nuestro caso este driver se ejecuta sobre una Raspberry Pi3 con raspbian a la que hemos instalado el entorno JdeRobot y el driver \texttt{APM Server}, e ir\'a abordo del robot f\'isico conectado por puerto serie a la placa estabilizadora (APM).

\section{Diseño} 
\label{sec:diseno_apms}

El driver tiene que cubrir las siguientes funcionalidades:
\begin{itemize}
\item Debe sebe ser capaz de conectar con dispositivos f\'isicos de estabilizaci\'on como APM 2.8 o PixHawk, as\'i como al simulador SITL
\item Debe acceder a los sensores del robot a\'ereo, interpretar sus datos y servirlos en forma de interfaz ICE a las aplicaciones de control.
\item Debe ser capar de recibir órdenes y comandos a trav\'es de interfaces JdeRobot codificarlas y enviarlas los actuadores del robot a\'ereo.
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.65]{img/diseno.jpg}
  \caption{Diseño de entradas y salidas de \texttt{APM Server}}
  \label{fig:diseno_apms_caja_negra}
\end{figure}


Se ha diseñado y programado un componente de JdeRobot en Python que hace de driver y que satisface esos requirimientos. Se ha organizado en tres capas, desde la m\'as cercana al hardware hasta la m\'as cercana a las aplicaciones:
\begin{itemize}
\item Capa de comunicaci\'on con el dispositivo APM. Esta capa se encarga de la comunicaci\'on del driver con el dispositivo APM a trav\'es del protocolo de comunicaci\'on MavLink.
\item Capa de negocio e interpretaci\'on. En esta capa el driver transforma los comandos MAVLink y los interfaces JdeRobot interpretando la informaci\'on de ambas capas y haciendo \'esta legible en ambos sentidos.
\item Capa de comunicaci\'on con aplicaciones JdeRobot. En \'esta capa se materializan los servicios ICE necesarios para la recepci\'on y env\'io desde/hacia las aplicaciones, a través de los interfaces ICE que implementa.
\end{itemize}

\begin{figure}[H]
\centering{
   \label{f:diseno_apms_caja_trans}
    \includegraphics[width=0.7\textwidth]{img/diseno1.jpg}}
    \caption{Bloques del driver \texttt{APM Server}}
  
\end{figure}

\section{Bloque de conexi\'on MAVLink}

En la capa de comunicaci\'on con el APM es d\'onde se van a recibir comandos MAVLink desde la placa estabilizadora y
autopiloto y se van a consumir y enviar los siguientes mensajes. Para cada uno de ellos se detalla su nombre, su sintaxis y su semántica.

\subsection{Listado de mensajes MAVLink}
\begin{itemize}
\item Mensaje RAW\_IMU, donde recibimos la informaci\'on de los aceler\'ometros gir\'oscopos y magnetr\'ometros en crudo para el \texttt{NavData}.
{\scriptsize
\begin{verbatim}
<message id="27" name="RAW_IMU">
  <description>The RAW IMU readings for the usual 9DOF sensor setup. 
    This message should alwaysCcontain the true raw values without any
    scaling to allow data capture and system debugging.</description>
  <field type="uint64_t" name="time_usec" units="us">Timestamp 
    (microseconds since UNIX epoch or microseconds since system boot)</field>
  <field type="int16_t" name="xacc">X acceleration (raw)</field>
  <field type="int16_t" name="yacc">Y acceleration (raw)</field>
  <field type="int16_t" name="zacc">Z acceleration (raw)</field>
  <field type="int16_t" name="xgyro">Angular speed around X axis (raw)</field>
  <field type="int16_t" name="ygyro">Angular speed around Y axis (raw)</field>
  <field type="int16_t" name="zgyro">Angular speed around Z axis (raw)</field>
  <field type="int16_t" name="xmag">X Magnetic field (raw)</field>
  <field type="int16_t" name="ymag">Y Magnetic field (raw)</field>
  <field type="int16_t" name="zmag">Z Magnetic field (raw)</field>
</message>
\end{verbatim}}
Un ejemplo de mensaje recibido ser\'ia: 
RAW\_IMU \{time\_usec : 480794000, xacc : 244, yacc : 0, zacc : -968, xgyro : 1, ygyro : 1, zgyro : 1, xmag : 335, ymag : 60, zmag : -452\}

\item Mensaje VFR\_HUD, donde se  presentan los t\'ipicos datos de navegaci\'on en ala fija como, la velocidad en el aire, velocidad en tierra o la velocidad de ascenso. De aqu\'i obtendremos la altitud
{\scriptsize
\begin{verbatim}
<message id="74" name="VFR_HUD">
  <description>Metrics typically displayed on a HUD for fixed wing aircraft</description>
  <field type="float" name="airspeed" units="m/s">Current airspeed in m/s</field>
  <field type="float" name="groundspeed" units="m/s">Current ground speed in m/s</field>
  <field type="int16_t" name="heading" units="deg">Current heading in degrees, 
    in compass units (0..360, 0=north)</field>
  <field type="uint16_t" name="throttle" units="%">Current throttle setting in 
    integer percent, 0 to 100</field>
  <field type="float" name="alt" units="m">Current altitude (MSL), in meters</field>
  <field type="float" name="climb" units="m/s">Current climb rate in meters/second</field>
</message>
\end{verbatim}}
Un ejemplo de mensaje recibido ser\'ia: VFR\_HUD \{airspeed : 0.022061701864004135, groundspeed : 0.08534427732229233, heading : 356, throttle : 0, alt : 584.1099853515625, climb : -0.33149993419647217\} 
\item Mensaje ATTITUDE, donde recibimos la actitud del avi\'on.
{\scriptsize
\begin{verbatim}
<message id="30" name="ATTITUDE">
  <description>The attitude in the aeronautical frame 
    (right-handed, Z-down, X-front, Y-right).</description>
  <field type="uint32_t" name="time_boot_ms" units="ms">Timestamp 
    (milliseconds since system boot)</field>
  <field type="float" name="roll" units="rad">Roll angle (rad, -pi..+pi)</field>
  <field type="float" name="pitch" units="rad">Pitch angle (rad, -pi..+pi)</field>
  <field type="float" name="yaw" units="rad">Yaw angle (rad, -pi..+pi)</field>
  <field type="float" name="rollspeed" units="rad/s">Roll angular speed (rad/s)</field>
  <field type="float" name="pitchspeed" units="rad/s">Pitch angular speed (rad/s)</field>
  <field type="float" name="yawspeed" units="rad/s">Yaw angular speed (rad/s)</field>
</message>
\end{verbatim}}
Un ejemplo de mensaje recibido ser\'ia:
ATTITUDE \{time\_boot\_ms : 480794, roll : -0.005003694910556078, pitch : 0.2430807501077652, yaw : -0.06844368577003479, rollspeed : -0.0010073177982121706, pitchspeed : -0.0008516315137967467, yawspeed : -0.0006909647490829229\}

\item Mensaje SYS\_STATUS, donde se obtiene el nivel de bater\'ia.
{\scriptsize
\begin{verbatim}
<message id="1" name="SYS_STATUS">
  <description>The general system state. If the system is following the MAVLink standard, 
    the system state is mainly defined by three orthogonal states/modes: The system mode, 
    which is either LOCKED (motors shut down and locked), MANUAL (system under RC control), 
    GUIDED (system with autonomous position control, position setpoint controlled manually) 
    or AUTO (system guided by path/waypoint planner). 
    The NAV_MODE defined the current flight state: 
      LIFTOFF (often an open-loop maneuver), 
      LANDING
      WAYPOINTS
      VECTOR. 
    This represents the internal navigation state machine. The system status shows wether 
    the system is currently active or not and if an emergency occured. 
    During the CRITICAL and EMERGENCY states the MAV is still considered to be active, 
    but should start emergency procedures autonomously. After a failure occured it should 
    first move from active to critical to allow manual intervention and then move to 
    emergency after a certain timeout.</description>
  <field type="uint32_t" name="onboard_control_sensors_present" enum="MAV_SYS_STATUS_SENSOR" 
    display="bitmask" print_format="0x%04x">
    Bitmask showing which onboard controllers and sensors are present. Value of 0: not 
    present. Value of 1: present. Indices defined by ENUM MAV_SYS_STATUS_SENSOR</field>
  <field type="uint32_t" name="onboard_control_sensors_enabled" enum="MAV_SYS_STATUS_SENSOR" 
    display="bitmask" print_format="0x%04x">
    Bitmask showing which onboard controllers and sensors are enabled:  Value of 0: 
    not enabled. Value of 1: enabled. Indices defined by ENUM MAV_SYS_STATUS_SENSOR</field>
  <field type="uint32_t" name="onboard_control_sensors_health" enum="MAV_SYS_STATUS_SENSOR" 
    display="bitmask" print_format="0x%04x">
    Bitmask showing which onboard controllers and sensors are operational or have an error:  
    Value of 0: not enabled. Value of 1: enabled. 
    Indices defined by ENUM MAV_SYS_STATUS_SENSOR</field>
  <field type="uint16_t" name="load" units="d%">Maximum usage in percent of the mainloop 
    time, (0%: 0, 100%: 1000) should be always below 1000</field>
  <field type="uint16_t" name="voltage_battery" units="mV">Battery voltage, in millivolts 
    (1 = 1 millivolt)</field>
  <field type="int16_t" name="current_battery" units="cA">Battery current, in 
    10*milliamperes (1 = 10 milliampere), -1: autopilot does not measure the current</field>
  <field type="int8_t" name="battery_remaining" units="%">Remaining battery energy: 
    (0%: 0, 100%: 100), -1: autopilot estimate the remaining battery</field>
  <field type="uint16_t" name="drop_rate_comm" units="c%">Communication drops in percent, 
    (0%: 0, 100%: 10'000), (UART, I2C, SPI, CAN), dropped packets on all links 
    (packets that were corrupted on reception on the MAV)</field>
  <field type="uint16_t" name="errors_comm">Communication errors (UART, I2C, SPI, CAN), 
    dropped packets on all links (packets that were corrupted on reception on the MAV)</field>
      <field type="uint16_t" name="errors_count1">Autopilot-specific errors</field>
      <field type="uint16_t" name="errors_count2">Autopilot-specific errors</field>
      <field type="uint16_t" name="errors_count3">Autopilot-specific errors</field>
      <field type="uint16_t" name="errors_count4">Autopilot-specific errors</field>
</message>
\end{verbatim}}
Un mensaje recibido ser\'ia: 
SYS\_STATUS \{onboard\_control\_sensors\_present : 23198783, onboard\_control\_sensors\_enabled : 23198783, onboard\_control\_sensors\_health : 24247359, load : 0, voltage\_battery : 12587, current\_battery : 0, battery\_remaining : 100, drop\_rate\_comm : 0, errors\_comm : 0, errors\_count1 : 0, errors\_count2 : 0, errors\_count3 : 0, errors\_count4 : 0\}
\item Mensaje SCALED\_PRESSURE, donde obtendremos la presi\'on absoluta y la temperatura.
{\scriptsize
\begin{verbatim}
<message id="29" name="SCALED_PRESSURE">
  <description>The pressure readings for the typical setup of one absolute and differential 
      pressure sensor. The units are as specified in each field.</description>
  <field type="uint32_t" name="time_boot_ms" units="ms">Timestamp 
      (milliseconds since system boot)</field>
  <field type="float" name="press_abs" units="hPa">Absolute pressure (hectopascal)</field>
  <field type="float" name="press_diff" units="hPa">Differential pressure 1 </field>
  <field type="int16_t" name="temperature" units="cdegC">Temperature measurement 
     (0.01 degrees celsius)</field>
</message>
\end{verbatim}}
Un mensaje recibido ser\'ia: 
SCALED\_PRESSURE \{time\_boot\_ms : 480794, press\_abs : 945.0001831054688, press\_diff : 0.021015625447034836, temperature : 2600\}
\item Mensaje WIND, donde se obtienen las lecturas del viento estimadas.
{\scriptsize
\begin{verbatim}
<message id="168" name="WIND">
    <description>Wind estimation</description>
    <field name="direction" type="float">wind direction that wind is coming from </field>
    <field name="speed" type="float">wind speed in ground plane (m/s)</field>
    <field name="speed_z" type="float">vertical wind speed (m/s)</field>
</message>
\end{verbatim}}
Un mensaje recibido ser\'ia: 
WIND \{direction : -179.99998474121094, speed : 0.0, speed\_z : 0.0\}
\item Mensaje GLOBAL\_POSITION\_INT, donde obtendremos la posici\'on GPS.
{\scriptsize
\begin{verbatim}
<message id="33" name="GLOBAL_POSITION_INT">
  <description>The filtered global position (e.g. fused GPS and accelerometers). 
    The position is in GPS-frame (right-handed, Z-up). It is designed as scaled integer 
    message since the resolution of float is not sufficient.</description>
  <field type="uint32_t" name="time_boot_ms" units="ms">Timestamp 
    (milliseconds since system boot)</field>
  <field type="int32_t" name="lat" units="degE7">Latitude, expressed as degrees * 1E7</field>
  <field type="int32_t" name="lon" units="degE7">Longitude, expressed as degrees * 1E7</field>
  <field type="int32_t" name="alt" units="mm">Altitude in meters, expressed as 
    * 1000 (millimeters), AMSL (not WGS84 - note that virtually all GPS modules provide 
    the AMSL as well)</field>
  <field type="int32_t" name="relative_alt" units="mm">Altitude above ground in meters,
    expressed as * 1000 (millimeters)</field>
  <field type="int16_t" name="vx" units="cm/s">Ground X Speed (Latitude, positive north), 
    expressed as m/s * 100</field>
  <field type="int16_t" name="vy" units="cm/s">Ground Y Speed (Longitude, positive east), 
    expressed as m/s * 100</field>
  <field type="int16_t" name="vz" units="cm/s">Ground Z Speed (Altitude, positive down), 
    expressed as m/s * 100</field>
  <field type="uint16_t" name="hdg" units="cdeg">Vehicle heading (yaw angle) in degrees 
    * 100, 0.0..359.99 degrees. If unknown, set to: UINT16_MAX</field>
</message>
\end{verbatim}}
Un mensaje recibido ser\'ia: 
GLOBAL\_POSITION\_INT \{time\_boot\_ms : 480614, lat : -353632612, lon : 1491652301, alt : 584110, relative\_alt : -179, vx : 0, vy : 0, vz : 0, hdg : 35608\}
\item Mensaje MISSION\_ITEM, son objetos de misi\'on, en ellos se mandan los puntos de paso o comandos, como fijar una velocidad o una altitud o bien aterrizar o despegar en funci\'on del par\'ametro \texttt{command}.
{\scriptsize
\begin{verbatim}
<message id="39" name="MISSION_ITEM">
  <description>Message encoding a mission item. This message is emitted to announce the 
    presence of a mission item and to set a mission item on the system. The mission item 
    can be either in x, y, z meters (type: LOCAL) or x:lat, y:lon, z:altitude. 
    Local frame is Z-down, right handed (NED), global frame is Z-up, right handed (ENU). 
    See also http://qgroundcontrol.org/mavlink/waypoint_protocol.</description>
  <field type="uint8_t" name="target_system">System ID</field>
  <field type="uint8_t" name="target_component">Component ID</field>
  <field type="uint16_t" name="seq">Sequence</field>
  <field type="uint8_t" name="frame" enum="MAV_FRAME">The coordinate system of the MISSION. 
    see MAV_FRAME in mavlink_types.h</field>
  <field type="uint16_t" name="command" enum="MAV_CMD">The scheduled action for the MISSION. 
    see MAV_CMD in common.xml MAVLink specs</field>
  <field type="uint8_t" name="current">false:0, true:1</field>
  <field type="uint8_t" name="autocontinue">autocontinue to next wp</field>
  <field type="float" name="param1">PARAM1, see MAV_CMD enum</field>
  <field type="float" name="param2">PARAM2, see MAV_CMD enum</field>
  <field type="float" name="param3">PARAM3, see MAV_CMD enum</field>
  <field type="float" name="param4">PARAM4, see MAV_CMD enum</field>
  <field type="float" name="x">PARAM5 / local: x position, global: latitude</field>
  <field type="float" name="y">PARAM6 / y position: global: longitude</field>
  <field type="float" name="z">PARAM7 / z position: global: altitude (relative or absolute,
    depending on frame.</field>
  <extensions/>
  <field type="uint8_t" name="mission_type" enum="MAV_MISSION_TYPE">Mission type, 
    see MAV_MISSION_TYPE</field>
</message>
\end{verbatim}}
Un mensaje enviado ser\'ia: MISSION\_ITEM \{target\_system : 1, target\_component : 1, seq : 0, frame : 3, command : 16, current : 0, autocontinue : 0, param1 : 0, param2 : 10, param3 : 0, param4 : 0, x : 40.33024215698242, y : -3.8008816242218018, z : 40.0\}
\item Mensaje MISSION\_REQUEST, donde el estabilizador nos requiere el siguiente MISSION\_ITEM. Este comando se recibe una vez enviado al APM nuestra intenci\'on de enviarle mensajes de misi\'on y cuántos objetos vamos a enviarle con el comando MISSION\_COUNT. También se recibe este comando tras la recepci\'on de alguno de estos MISSION\_ITEM, como confirmación de la recepción de éste, y solicitando el siguiente objeto de misión.
{\scriptsize
\begin{verbatim}
<message id="40" name="MISSION_REQUEST">
  <description>Request the information of the mission item with the sequence number seq. 
    The response of the system to this message should be a MISSION_ITEM message. 
    http://qgroundcontrol.org/mavlink/waypoint_protocol</description>
  <field type="uint8_t" name="target_system">System ID</field>
  <field type="uint8_t" name="target_component">Component ID</field>
  <field type="uint16_t" name="seq">Sequence</field>
  <extensions/>
  <field type="uint8_t" name="mission_type" enum="MAV_MISSION_TYPE">Mission type, 
    see MAV_MISSION_TYPE</field>
</message>
\end{verbatim}}
Un mensaje recibido ser\'ia: MISSION\_REQUEST \{target\_system : 0, target\_component : 0, seq : 2\} En este mensaje el APM nos estar\'ia solicitando el item de misi\'on 3, ya que empieza solicitando el 0. Este mensaje termina al llegar al m\'aximo de \texttt{mission item} que se ha indicado que le se le va a mandar.

\item Mensaje MISSION\_COUNT, \'este comando se env\'ia para comenzar una comunicaci\'on con el APM con el objetivo de enviarle una misi\'on.
{\scriptsize
\begin{verbatim}
<message id="44" name="MISSION_COUNT">
  <description>This message is emitted as response to MISSION_REQUEST_LIST by the MAV
    and to initiate a write transaction. The GCS can then request the individual
    mission item based on the knowledge of the total number of MISSIONs.</description>
  <field type="uint8_t" name="target_system">System ID</field>
  <field type="uint8_t" name="target_component">Component ID</field>
  <field type="uint16_t" name="count">Number of mission items in the sequence</field>
  <extensions/>
  <field type="uint8_t" name="mission_type" enum="MAV_MISSION_TYPE">Mission type, 
    see MAV_MISSION_TYPE</field>
</message>
\end{verbatim}}
Un mensaje enviado ser\'ia: MISSION\_COUNT \{target\_system : 0, target\_component : 0, count : 3\} Donde comunicamos al APM nuestra intenci\'on de enviarle 3 objetos de misi\'on.

\item Mensaje MISSION\_CLEAR\_ALL, este comando sirve para limpiar el APM de misiones, si \'este ten\'ia alguna misi\'on previa se desecha.
{\scriptsize
\begin{verbatim}
<message id="45" name="MISSION_CLEAR_ALL">
  <description>Delete all mission items at once.</description>
  <field type="uint8_t" name="target_system">System ID</field>
  <field type="uint8_t" name="target_component">Component ID</field>
  <extensions/>
  <field type="uint8_t" name="mission_type" enum="MAV_MISSION_TYPE">Mission type, 
    see MAV_MISSION_TYPE</field>
</message>
\end{verbatim}}
Un mesaje enviado ser\'ia MISSION\_CLEAR\_ALL \{target\_system : 0, target\_component : 0\}
\end{itemize}

\subsection{Conexi\'on y configuraci\'on de la comunicación con el APM}

De cara a facilitar la implementaci\'on del software, ahorrarnos crear los comandos MAVLink a mano, y hacer el c\'odigo m\'as legible, utilizaremos la librer\'ia \texttt{Pymavlink}. 

Las importaciones que se utilizan son las siguientes:

\begin{python}[
    basicstyle=\scriptsize, %or \small or \footnotesize etc.
]
from pymavlink import mavutil, quaternion, mavwp
from pymavlink.dialects.v10 import ardupilotmega as mavlink
\end{python}

Se importa \pythoninline{mavutil} y \pythoninline{ardupilotmega} para obtener los interfaces en python desde \pythoninline{pymavlink} y \pythoninline{mavwp} y \pythoninline{quaternion} como soporte a la transformaci\'on a cuaterniones y viceversa.


El primer paso para interactuar con el APM es el proceso de conexi\'on (Figura 4.3). En el comando de conexi\'on se le indica una de las siguiente tuplas:
\begin{itemize}
\item Dispositivo\_serie, velocidad de conexión en baudios (Línea 9 de la Figura 4.3).
\item Tipo\_de\_puerto:IP:puerto, velocidad de conexión en baudios. En este caso los baudios son obligatorios pero se desechan (Línea 10 de la Figura 4.3). 
\end{itemize}

\begin{figure}
\begin{python}[	
    basicstyle=\scriptsize, %or \small or \footnotesize etc.
]
class Server:

    def __init__(self, port, baudrate):
        self.master = mavutil.mavlink_connection(port, baudrate, autoreconnect=True)
        print('Connection established to device')
        self.master.wait_heartbeat()
        print("Heartbeat Recieved")

#test = Server("/dev/ttyUSB0", 57600) # Connection to the real APM device
test = Server("udp:192.168.1.133:14558",57600) # Connection to SITL
\end{python}
\caption{Conexión con el APM}
\end{figure}

Con estas l\'ineas se inicia el servidor de \texttt{APM Server} y se realiza la conexi\'on con el dispositivo APM o con el simulador. Se puede apreciar c\'omo se realiza la conexi\'on en el m\'etodo \pythoninline{__init__}  del servidor. Y la creaci\'on de un objeto de servidor, que conecta con el dispositivo real y en el comentario de la instrucción que hace lo propio con el dispositivo simulado en SITL (Línea 9 de la Figura 4.3).


Una vez conectado, se debe indicar al APM qu\'e mensajes se quieren recibir y la frecuencia con la que se quieren recibir (Figura 4.4). Debido a que se trata de un prototipo y se trata de extraer el m\'aximo potencial del mismo, se indicar\'a el juego de instrucciones completo y una frecuencia de moderada a alta 50 hz, lo m\'aximo que soporta el APM adquirido.

\begin{figure}
\begin{python}[
    basicstyle=\scriptsize, %or \small or \footnotesize etc.
]
RATE = 50
self.master.mav.request_data_stream_send(self.master.target_system,
                                         self.master.target_component,
                                         mavutil.mavlink.MAV_DATA_STREAM_ALL,
                                         RATE, 1)
\end{python}
\caption{configuración de la conexión}
\end{figure}
Desde este momento se van a estar volcando a un \textit{buffer} los mensajes que manda nuestro APM, con lo que ahora ya se puede acceder a ellos desde Python para procesarlos.

\subsection{Lectura de datos del APM}

Una vez establecida la conexi\'on y fijados los par\'ametros de configuraci\'on ya se puede acceder a los datos. Para \'esto en el bloque de transformaci\'on se levantar\'a el siguiente hilo que procesar\'a los mensajes.
Con el fin de evitar una desconexión con el APM envíamos periódicamente \texttt{heartbeat} al mismo. \texttt{heartbeat} es un comando que sirve para conocer si el dispositivo esta disponible y atiende peticiones, de esta forma nos aseguramos cada cierto tiempo que el APM continúa activo. 

\begin{figure}[H]
\begin{python}[
    basicstyle=\scriptsize, %or \small or \footnotesize etc.
]
MsgHandler = threading.Thread(target=self.mavMsgHandler, args=(self.master,), name='msg_Handler')
MsgHandler.start()


def mavMsgHandler(self, m):

    while True:
        msg = m.recv_msg()

	if time.time() - self.lastSentHeartbeat > 1.0:
	    self.master.mav.heartbeat_send(mavlink.MAV_TYPE_GCS, mavlink.MAV_AUTOPILOT_INVALID, 0, 0, 0)
	    self.lastSentHeartbeat = time.time()

	    # refresh the attitude
	    self.refreshAPMPose3D()
	    self.refreshAPMnavdata()

	elif msg is None or msg.get_type() == "BAD_DATA":
	    time.sleep(0.01)
	    continue
\end{python}
\caption{Hilo de alimentación de mensajes MAVLink}
\end{figure}

\subsection{Env\'io de misiones al APM}

Con el fin de entender mejor la implementación del método \pythoninline{self.setMision(mission)}, en la Figura 4.6 podemos observar un gr\'afico de c\'omo es el protocolo de env\'io de misiones al APM a trav\'es de MAVLink, obtenido de la universidad University of Colorado Boulder \cite{colorado:_mission}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.70]{img/waypoint-protocol-sendlist.png}
  \caption{Protocolo de env\'io de misiones MAVLink}
  \label{fig:misiones_mavlink}
\end{figure}

Para abordar el problema e implementar dicho protocolo partimos de la soluci\'on que nos propone la Universidad de Colorado Boulder \cite{colorado:_mission} y la adaptamos a nuestras necesidades, recibiendo de entrada un objeto de la interfaz \texttt{mission} de JdeRobot, transform\'andolo y envi\'andolo como misi\'on de MavLink al APM. Tambi\'en revisamos si se han enviado \'ordenes de despegue y aterrizaje y en el caso de que as\'i sea añadimos sus \textit{mission item} al principio o al final de la misi\'on. En este punto se entrelazan la capa de interpretaci\'on con la capa de comunicaci\'on con el APM

\begin{python}[
    basicstyle=\scriptsize, %or \small or \footnotesize etc.
]
    def setMission(self, mission):

        wp = mavwp.MAVWPLoader()
        seq = 1
        frame = mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT
        radius = 10
        pose3Dwaypoints = mission.mission
        N = len(pose3Dwaypoints)
        if (self.extra.takeOffDecision):
            navData = pose3Dwaypoints[seq-1]
            toff = mavutil.mavlink.MAVLink_mission_item_message(self.master.target_system,
                                                                self.master.target_component,
                                                                seq,
                                                                frame,
                                                                mavutil.mavlink.MAV_CMD_NAV_TAKEOFF, # 22
                                                                0, 0, 0, radius, 0, 0,
                                                                navData.x, navData.y, navData.h)
			...

        for i in range(N):
            navData = pose3Dwaypoints[i]
            wayPoint_tmp =mavutil.mavlink.MAVLink_mission_item_message(self.master.target_system,
                                                                self.master.target_component,
                                                                seq,
                                                                frame,
                                                                mavutil.mavlink.MAV_CMD_NAV_WAYPOINT, # 16
                                                                0, 0, 0, radius, 0, 0,
                                                                navData.x, navData.y, navData.h)
            ...

        if (self.extra.landDecision):
            navData = pose3Dwaypoints[N-1]
            land = mavutil.mavlink.MAVLink_mission_item_message(self.master.target_system,
                                                                self.master.target_component,
                                                                seq,
                                                                frame,
                                                                mavutil.mavlink.MAV_CMD_NAV_LAND, # 21
                                                                0, 0, 0, radius, 0, 0,
                                                                navData.x, navData.y, navData.h)
            ...

        self.master.waypoint_clear_all_send()
        self.master.waypoint_count_send(wp.count())

        for i in range(wp.count()):
            msg = self.master.recv_match(type=['MISSION_REQUEST'], blocking=True)
            self.master.mav.send(wp.wp(i))

        self.master.arducopter_arm()
        self.master.set_mode_auto() # start mission
        ...

\end{python}


Como se puede observar en el c\'odigo, en caso de tener \texttt{takeOffDecision} o \texttt{landDecision} activos en el atributo \pythoninline{extra} (enlazado a trav\'es de ICE) se añaden los comandos de despegue y aterrizaje, que interpreta son el primero y el \'ultimo. 
Se puede observar tambi\'en al final del c\'odigo cómo se arman los motores con la l\'inea \pythoninline{self.master.arducopter_arm()}
y cómo se pone el dispositivo en modo autom\'atico con \pythoninline{self.master.set_mode_auto()}.
El driver est\'a implementado para ejecutarse en modo ``auto''. Se puede ejecutar tambi\'en en modo ``guided'' pero el modo ``auto'', a diferencia del ``guided'', no inhabilita la radio y se puede recuperar en cualquier momento el control del robot a\'ereo. Si bien hay que decir que el modo ``guided'' o modo guiado es m\'as preciso que el modo ``auto''.

\section{Capa de conexi\'on con aplicaciones JdeRobot}


En esta capa se sirven y reciben los objetos de interfaces ICE JdeRobot a través de los cuales se habla con la aplicación (que es el destino final de la información de sensores y la fuente original de los comandos de actuación para el avión), de modo que sean entendibles por el resto del driver.
Para esto se han implementado cuatro hilos, uno por cada interfaz JdeRobot que se sirve o se recibe. En cada uno de estos hilos se levanta un servidor ICE para recibir o servir objetos de interfaces JdeRobot mapeando atributos de la clase del servidor a estos servicios ICE. La figura 4.7 muestra dos ejemplos.

Los 4 interfaces que van a ser servidos desde \texttt{APM Server} son: \texttt{Pose3D}, \texttt{NavData}, \texttt{Extra} y \texttt{Mission}. \texttt{Pose3D}, \texttt{NavData} y \texttt{Extra} se describieron en la sección 3.1.1. Adicionalmente a \'estos se ha desarrollado un nuevo interfaz para dar soporte al uso de misiones, el interfaz \texttt{mission}.
{\scriptsize
\begin{verbatim}
class Pose3DData  //we consumes Pose3DData
  {
	float x;  //latitude
	float y;  //longitude
	float z;  //altitude
	float h;  //not used now
	float q0; //quaternion component 1
	float q1; //quaternion component 2
	float q2; //quaternion component 3
	float q3; //quaternion component 4
  };

  ["python:seq:list"] sequence<Pose3DData> PoseSequence; // list of Pose3DData
  /**
  * Mission data information, Pose3DData sequence. 
  */
  class MissionData
  {
    PoseSequence mission;
  };

  /** 
   * Interface to the Mission.
   */
  interface Mission
  {
    idempotent MissionData getMissionData();
    int setMissionData(MissionData data);
  };
\end{verbatim}}

\begin{figure}[H]
\begin{python}[
    basicstyle=\scriptsize, %or \small or \footnotesize etc.
]
 
        PoseTheading = threading.Thread(target=self.openPose3DChannel, args=(self.pose3D,), name='Pose_Theading')
        PoseTheading.daemon = True
        PoseTheading.start()

        ...

        ExtraTheading = threading.Thread(target=self.openExtraChannel, args=(self.extra,), name='Extra_Theading')
        ExtraTheading.daemon = True
        ExtraTheading.start()

\end{python}
\caption{Creación y arranque de los hilos encargados de servir los interfaces ICE}
\end{figure}


Cada hilo abre un servicio ICE, como todos los servicios ICE que implementamos son similares podremos como ejemplo de implementaci\'on el servicio de Pose3D.
Como se puede apreciar en esta capa tenemos cuatro hilos de ejecuci\'on simult\'aneos sirviendo objetos ICE a trav\'es de servidores como el de la FIgura 4.8. Como se ha descrito en la sección 4.2.3 en paralelo se tiene otro hilo que procesa los mensajes recibidos del APM o SITL, los interpreta y cumplimenta estos objetos ICE, con lo que se ha tenido que cuidar el c\'odigo para evitar problemas de concurrencia. En las Figuras 4.7 y 4.8 se puede observar cómo se crea un hilo asocado a un servidor ICE y cómo se implementa un servidor ICE respectivamente.

\begin{figure}[H]
\begin{python}[
    basicstyle=\scriptsize, %or \small or \footnotesize etc.
]
    def openPose3DChannel(self, pose3D):
        status = 0
        ic = None
        # recovering the attitude
        Pose2Tx = pose3D
        try:
            ic = ICE.initialize(sys.argv)
            adapter = ic.createObjectAdapterWithEndpoints("Pose3DAdapter", "default -p 9998")
            object = Pose2Tx
            # print object.getPose3DData()
            adapter.add(object, ic.stringToIdentity("ardrone_pose3d")) #ardrone_pose3d  Pose3D
            adapter.activate()
            ic.waitForShutdown()
        except:
            traceback.print_exc()
            status = 1
        if ic:
            # Clean up
            try:
                ic.destroy()
            except:
                traceback.print_exc()
                status = 1

        sys.exit(status)

\end{python}
\caption{Servidor ICE encargado de servir objetos de la interfaz Pose3D}
\end{figure}


\section{Bloque de interpretaci\'on}

En esta capa se interpretan los mensajes recibidos de una y otra capa y se transforman en el formato que sea necesario para que lo interprete la opuesta.

Al m\'etodo \pythoninline{setMission(mission)} se llama al recibir v\'ia ICE una misión. Para identificar cuándo llega, debido a que tenemos mapeadas las variables con ICE, se ha construido un \textit{listener} que valida el valor de la variable \texttt{mission} y si tiene contenido articula el m\'etodo \pythoninline{setMission(mission)}.

\begin{figure}
\begin{python}[
    basicstyle=\scriptsize, %or \small or \footnotesize etc.
]
    MissionListener = threading.Thread(target=self.missionListener, name='MissionListener')
    MissionListener.daemon = True
    MissionListener.start()

    def missionListener(self):
        '''
        Function who listen to a new mission revieved from ICE MissionChannel thread and send it to APM
        :return: None
        '''
        while True:
            if not self.mission.is_empty():
                self.lastMission = self.mission
                self.setMission(self.mission.getMissionData())
            time.sleep(1)

\end{python}
\caption{Código encargado de detectar misiones entrantes}
\end{figure}

Ya se visto c\'omo se realiza la transformaci\'on entre mensajes de misi\'on en el m\'etodo \pythoninline{setMission(mission)}, ahora nos centraremos en interpretar los mensajes recibidos del APM y crear los objetos de las interfaces que sean necesarias para su interpretaci\'on por las aplicaciones JdeRobot.
Esto lo realizamos principalmente en dos m\'etodos, \pythoninline{refreshAPMPose3D()} y \pythoninline{refreshAPMnavdata()}.
Estos m\'etodos se centran en buscar dentro de los paquetes recibidos del APM aquellos que contienen informaci\'on relevante, los descritos en la sección 4.2, analizarlos y extraer de ellos la informaci\'on necesaria y guardarla en objetos de interfaces JdeRobot para ser servidos a trav\'es de ICE.

\begin{figure}
\begin{python}[
    basicstyle=\scriptsize, %or \small or \footnotesize etc.
]
    def refreshAPMPose3D(self):
        # get attitude of APM
        if 'ATTITUDE' not in self.master.messages:
            self.attitudeStatus = 1
            q=[0,0,0,0]
        else:
    	    attitude = self.master.messages['ATTITUDE']
            yaw = getattr(attitude,"yaw")
            pitch = getattr(attitude,"pitch") * -1
            roll = getattr(attitude,"roll")
            q = quaternion.Quaternion([roll, pitch, yaw])

        # get altitude of APM
        altitude = self.master.field('VFR_HUD', 'alt', None)
        if altitude is None:
            self.altitudeStatus = 1

        # get GPS position from APM
        latitude = 0
        longitude = 0
        if 'GPS_RAW_INT' not in self.master.messages:
            self.gpsStatus = 1
        else:
            gps = self.master.messages['GPS_RAW_INT']
            latitude = getattr(gps,"lat")/ 10e6
            longitude = getattr(gps,"lon") / 10e6
            self.GPS_fix_type = getattr(gps,"fix_type")

        # refresh the pose3D
        data = jderobot.Pose3DData
        data.x = latitude
        data.y = longitude
        data.z = altitude
        data.h = altitude
        data.q0 = q.__getitem__(0)
        data.q1 = q.__getitem__(1)
        data.q2 = q.__getitem__(2)
        data.q3 = q.__getitem__(3)
        self.pose3D.setPose3DData(data)

    
\end{python}
\caption{Código de ejemplo de \texttt{refreshAPMPose3D()} encargado de recorrer los mensajes y obtener los datos de actitud que se van a servir a través de Pose3D}
\end{figure}

En la Figura 4.10 se pueden observar los métodos que recogen la actitud, podemos ver cómo se buscan los mensajes que necesitamos para extraer la información de los sensores desde \pythoninline{self.master.messages}. En esta variable se vuelcan los datos de todos loscmensajes recibidos de la placa estabilizadora.

\cleardoublepage
\chapter{AUV Commander}

\texttt{UAV Commander} es una aplicaci\'on JdeRobot que hace las veces de estaci\'on de tierra o GCS (\textit{Ground Control Station}) para robots a\'ereos y que ha sido íntegramente desarrollada dentro de este proyecto.
Desde \texttt{UAV Commander} se pueden programar misiones, enviar \'estas al driver de un avión autónomo MAVLink, verificar el cumplimiento de las mismas y visualizar los sensores a bordo del robot a\'ereo. Sirve de cliente del driver desarrollado en la primera parte de este Proyecto Fin de Carrera y proporciona su primera validación experimental.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.40]{img/uavc_comp.png}
  \caption{Vista general del interfaz gráfico de \texttt{UAV Commander}}
  \label{fig:uavc_ppal}
\end{figure}

\section{Diseño} 
\label{sec:diseno_uavc}


La aplicaci\'on debe cubrir las siguientes funcionalidades:

\begin{enumerate}
\item Presentar toda la informaci\'on sensorial que nos ofrezca el driver.
\begin{itemize}
	\item Actitud
	\item C\'amaras de abordo
\end{itemize}
\item Permitir la creaci\'on de misiones como secuencias de puntos por los que debe pasar el drone, ubicándolos dentro de un mapa geo-referenciado.
\item Enviar misiones a trav\'es de interfaces ICE al driver de JdeRobot que ofrece acceso al avión y es capaz de materializar esas misiones, de gobernar al avión para que las cumpla.
\item Validar el cumplimiento de misiones con algún ejemplo en el avión simulado.
\end{enumerate}

\begin{figure}[H]
\centering{
   \label{f:diseno_uavc_caja_negra}
    \includegraphics[width=1.05\textwidth]{img/diseno2_uavc.jpg}}
    \caption{Diseño de caja negra de \texttt{UAV Commander}, entradas y salidas}
  
\end{figure}
La aplicación utiliza los interfaces \texttt{Pose3D}, \texttt{NavData}, \texttt{Extra}, \texttt{camera} y \texttt{mission} y recupera, o bien de Google o bien del Instituto Geográfico Nacional de España (IGN), el mapa geo-referenciado de esa zona. Estos datos se presentan por pantalla al usuario a través de las distintas ventanas que conforman el interfaz gráfico. 


La aplicaci\'on \texttt{UAV Commander} se estructura en 3 grandes bloques:
\begin{itemize}
\item El bloque de sensores es el encargado de recuperar de los servicios ICE, tratar y mostrar toda la información sensorial a bordo del avión: la cámara, el GPS, unidad inercial, etc.
\item El bloque de Localización es el encargado de obtener los mapas geo-referenciados de la zona del mundo donde va a trabajar el avión, realizar las transformaciones entre sistemas de coordenadas y mostrar de forma interactiva éstos a través de la interfaz gráfica de usuario.
\item Bloque de control de misiones es quien se encarga de facilitar la creación de misiones a través del interfaz gráfico de usuario y las envía a través de los servicios ICE involucrados. 
\end{itemize}

\begin{figure}[H]
\centering{
   \label{f:diseno_uavc_caja_trans}
    \includegraphics[width=0.7\textwidth]{img/diseno_uavc.jpg}}
    \caption{Bloques dentro de la aplicación \texttt{UAV Commander}}
  
\end{figure}

La aplicación contiene cinco hilos asociados a cada uno de los servicios ICE que utiliza y dos hilos más para actualizar en el interfáz gráfico de usuario la posición en el mapa del avión, y los datos recibidos de los sensores.

Cada uno de estos bloques se detalla a continuación en las secciones siguientes.




\section{Bloque de sensores} 
\label{sec:bloque_sensores_uavc}

En este bloque se implementan todos los servidores ICE necesarios para acceder a la informaci\'on sensorial y para enviar los interfaces necesarios para la actuaci\'on.
La aplicación \texttt{UAV Commander} utiliza 5 interfaces ICE a diferencia de los 4 que usa en el driver APM Server. Además de los que expone APM Server (a saber: \texttt{Pose3D} y \texttt{NavData} para recibir la posición medida por los sensores a bordo como el GPS y \texttt{Extra} y \texttt{Mission} para el envío de misiones con puntos de paso y maniobras de aterrizaje o despegue), también utiliza el interfaz \texttt{Camera} para recoger las imágenes y vídeo de la cámara de abordo del avión que se muestran en el interfaz gráfico.


\subsection{Cámara}

\texttt{UAV Commander} utiliza el interfaz \texttt{Camera} para obtener las imágenes y vídeos recogidos desde el driver y mostrarlos en su interfaz gráfico. Para esto primero se crea el servicio ICE y se asigna al interfaz de usuario, una vez hecho esto, cuando el usuario hace \textit{click} en el \textit{checkBox} de camera, se abre la ventana de camera. En esta pantalla se recogen los datos de imagen y se asignan al área designado a mostrar las imágenes tal y como se puede ver en la Figura 5.4

\begin{figure}[H]
\centering{
   \label{f:diseno_uavc_caja_trans}
    \includegraphics[width=0.5\textwidth]{img/ventana_camera.png}}
    \caption{Ventana Camera de \texttt{UAV Commander}}
  
\end{figure}

\begin{figure}[H]
\begin{python}
    ic = EasyIce.initialize(sys.argv)
    camera = CameraClient(ic, "UavViewer.Camera", True)
    screen.setCamera(camera)
\end{python}
\caption{Creación del servicio ICE \texttt{camera} y asignación al GUI}
\end{figure}



\subsection{Sensores de vuelo}
Para simplificar el proceso de creación y enlazado de los servicios ICE hemos optado por utilizar los servidores (en el sentido de servidores de ICE, sin atender al sentido en el que se transmiten los datos) ya implementadas en JdeRobot, \texttt{CameraClient}, \texttt{NavDataClient}, \texttt{Pose3DClient} y \texttt{Extra} y la clase \texttt{easyiceconfig}. De \'este modo este bloque queda simplificado y su inicializaci\'on queda relegada a unas pocas l\'ineas de c\'odigo como vemos en la Figura 5.7.


\begin{figure}[H]
\centering{
   \label{f:actitud_uavc}
    \includegraphics[width=1\textwidth]{img/actitud.png}}
    \caption{Ventana actitud y batería de \texttt{UAV Commander}}
  
\end{figure}
\begin{figure}
\begin{python}
    ic = EasyIce.initialize(sys.argv)
    camera = CameraClient(ic, "UavViewer.Camera", True)
    navdata = NavDataClient(ic, "UavViewer.Navdata", True)
    pose = Pose3DClient(ic, "UavViewer.Pose3D", True)
    extra = Extra(ic, "UavViewer.Extra")
    mission = MissionI(ic, "UavViewer.Mission")
    screen.setPose3D(pose)
    screen.set_initial_pose3D(pose3D)
    screen.setNavData(navdata)
    screen.setExtra(extra)
    screen.setMission(mission)
\end{python}
\caption{Creación de servicios ICE y asignación al GUI}
\end{figure}


\section{Bloque de Mapas}
\label{sec:bloque_mapas_uavc}

Este bloque es el encargado de la obtención de los mapas geo-referenciados y la interpretación de la posición actual sobre ellos. Este bloque realiza las conversiones de sistemas de referencia necesarios para inferir la posición de un punto geográfico en el planeta a partir de un píxel de la imagen y viceversa, así como las operaciones necesarias para representar ese punto en la imagen.

\subsection{Obtención de Mapas}

Para obtener los mapas geo-referenciados se ha recurrido al uso de WebMapService (WMS) que es el método más empleado para la obtención de estos mapas. Se han utilizado dos WMS como fuentes, el PNOA del Instituto Geogr\'afico Nacional (IGN) como fuente principal y Google como fuente secundaria.

Con el fin de abstraernos del montaje de la URL y env\'io y tratamiento de la respuesta utilizaremos la librer\'ia OWSLib\footnote{\url{https://geopython.github.io/OWSLib/}}, en su versión 0.14.0, que nos ayudar\'a con \'estos tr\'amites. 
En el c\'odigo mostrado en la Figura 5.8 podemos observar cómo recuperamos un nuevo mapa del IGN\footnote{Instituto geogr\'afico nacional}, que publica uno de sus numerosos WebMapService (WMS) en `http://www.ign.es/wms-inspire/pnoa-ma'.
Para obtener un mapa geo-referenciado necesitamos principalmente cuatro cosas: la posición, el \textit{bounding box}, el tamaño de la imagen y la proyecci\'on a utilizar. 
Primero obtenemos el \textit{bounding box}. El \textit{bounding box} es la zona geogr\'afica que vamos a recuperar, se calcula a partir de la posici\'on GPS central y, en nuestro caso un radio. El c\'odigo para calcularlo podemos observarlo en el método \texttt{getBoundingBox(lat, lon, distance)} de la Figura 5.10.

Una vez obtenido el \textit{bounding box} \'unicamente rellenamos el resto parámetros del objeto WMS y lanzamos la petición. El WMS devuelve una imagen en bytes a la que debemos añadir el \textit{disclaimer} que nos indica el IGN para poder utlizar los mapas de forma gratuita.
Creamos la petición WMS con la URL del WMS PNOA y asignamos los parámetros necesarios para obtener el mapa. Al final montamos la imagen geo-referenciada, que en nuestro caso ser\'a un diccionario con la imagen en OpenCV, el \textit{bounding box}, y el tamaño de la imagen.

\begin{figure}
\begin{python}
def retrieve_new_map(lat, lon, radius, width, heigth):
    bbox = getBoundingBox(lat, lon, radius)
    wms = WebMapService('http://www.ign.es/wms-inspire/pnoa-ma', version='1.3.0')
    img = wms.getmap(layers=['OI.OrthoimageCoverage'], 
                     styles=['default'],
                     srs='EPSG:4326',
                     bbox=(bbox),
                     size=(width, heigth),
                     format='image/png',
                     transparent=True)
    ...
    ImageUtils.prepareInitialImage(img.read(), width, heigth)
    opencv_image = cv2.imread("images/imageWithDisclaimer.png", 1)
    image = {'bytes': opencv_image, 'bbox': bbox, 'size': (width, heigth)}
    return image
\end{python}
    \caption{Método \texttt{retrieve\_new\_map(lat, lon, radius, width, heigth)}}
\end{figure}

\begin{figure}
\begin{python}
def getBoundingBox(lat, lon, distance):
    radValues = from_degrees(lat, lon)
    rad_dist = distance / EARTH_RADIUS_WGS84
    min_lat = radValues[0] - rad_dist
    max_lat = radValues[0] + rad_dist
    if min_lat > MIN_LAT and max_lat < MAX_LAT:
        delta_lon = asin(sin(rad_dist) / cos(radValues[1]))
        min_lon = radValues[1] - delta_lon
        if min_lon < MIN_LON:
            min_lon += 2 * pi
        max_lon = radValues[1] + delta_lon
        if max_lon > MAX_LON:
            max_lon -= 2 * pi
    ...
    southWestPoint = from_radians(min_lat, min_lon)
    northEastpoint = from_radians(max_lat, max_lon)
    return southWestPoint[1], southWestPoint[0], northEastpoint[1], northEastpoint[0]
\end{python}
\caption{Métodos\texttt{getBoundingBox(lat, lon, distance)} encargado de calcular esquina inferior izquierda y esquina  superior derecha de un cuadrado de ancho \texttt{distance}}
\end{figure}

\subsection{Conversión de coordenadas}

La conversión de coordenadas o proyecci\'on se basa en ``ubicar'' un punto dado en un determinado sistema de coordenadas en otro sistema completamente distinto.
Los datos recibidos del GPS y los solicitados al WMS referencian a una proyección geográfica. La más extendida y que utilizamos aquí es la WGS84. Los datos del GPS los recibimos con este sistema de referencia y solicitamos los mapas en base a él. En nuestro caso trabajamos hasta con 3 sistemas de coordenadas distintos:(a) el de las im\'agenes con X[0, MAX\_WIDHT] creciente de izquierda a derecha e Y[0, MAX\_HEIGHT] creciente de arriba a abajo, (b)el sistema de coordenadas geogr\'aficas WGS84 y (c) el cartesiano. Las tres se mueven en \'ordenes de magnitud diferentes y un movimiento de $1 * 10^{-5}$ puede suponer varias decenas de p\'ixeles.
Podemos ver la proyecci\'on de posici\'on geogr\'afica a imagen en la Figura 5.11.


\begin{figure}[H]
\centering
\subfloat[Coordenadas cartesianas.]{
   \label{f:cartesianas}
    \includegraphics[width=0.31\textwidth]{img/cartesianas.png}}
    \subfloat[Coordenadas geograficas.]{
   \label{f:geograficas}
    \includegraphics[width=0.31\textwidth]{img/geograficas.jpg}}
    \subfloat[Coordenadas de una imagen]{
   \label{f:image}
    \includegraphics[width=0.5\textwidth]{img/image_coordinates.png}}
    \caption{Sistemas de referencia utilizados}
\end{figure}


Para abordar este problema, y dado que se ha trabajado con robots a\'ereos con una autonom\'ia relativamente reducida, se ha asumido que la tierra es plana en un área rectangular de menos de 2 kil\'ometros de ancho. Con \'esta convenci\'on conseguimos simplificar notablemente la complejidad del problema, que pr\'acticamente se convierte en una normalizaci\'on de un sistema a otro.
Si en un futuro se decidiese utilizar la aplicaci\'on para vuelos de mayor envergadura se deber\'ia tener en cuenta la curvatura de la tierra.
La proyecci\'on en \texttt{UAV Commander} se realiza en ambos sentidos, de imagen a posición geogr\'afica y de sistema de coordenadas a imagen.

\begin{figure}[H]
\begin{python}
def posCoords2Image(lonMin, latMin, lonMax, latMax, lat, lon, tamImageX, tamImageY):
    distCoordX = round(latMax - latMin,7)
    distCoordY = round(lonMax - lonMin, 7)
    x = ((latMax-lat) *(tamImageX))/distCoordX
    y = ((lon-lonMin) *(tamImageY))/distCoordY
    return round(y), round(x)
\end{python}
\caption{Método \texttt{posCoords2Image(lonMin, latMin, lonMax, latMax, lat, lon, tamImageX, tamImageY)}, encargado de calcular la proyección de un punto en coordenadas geográficas a la matriz de píxeles de la imagen que representa el mapa}
\end{figure}

\subsection{Visualización de mapas}
El mapa se presenta como un elemento más del control de misiones de la aplicación. Para visualizarlo por pantalla se ha creado una capa en PyQt5 con un QLabel que carga la imagen. Para realizar esta tarea de forma ágil y evitar el colapso de los recursos de la máquina donde se ejecuta, trabajamos con la biblioteca de visión artificial \texttt{OpenCV}, con lo que para mostrarla por pantalla hay que realizar ciertas transformaciones. El código encargado de mostrar por pantalla el mapa se puede ver en la Figura 5.12 y la apariencia de un mapa visualizado como ejemlplo en la Figura 5.13.

\begin{figure}[H]
\begin{python}
    def refreshImage(self):
        height, width, channel = self.cvImage.shape
        bytesPerLine = 3 * width
        qimg = QImage(self.im_to_show.data, width, height, bytesPerLine, QImage.Format_RGB888)
        qpm = QPixmap.fromImage(qimg)
        self.imageLabel.setPixmap(qpm)
\end{python}
\caption{Código encargado de mostrar por pantalla el mapa}
\end{figure}

\section{Bloque de misiones} 
\label{sec:bloque_misiones_uavc}

Este bloque es el encargado de facilitar la creación y envío de misiones. Las misiones se definen con una secuencia de puntos de paso (waypoints) que se quiere que el drone recorra, incluyendo un punto inicial de despegue y un punto final de aterrizaje. Ha de permitir crear misiones de forma interactiva además de identificar y convertir en objetos de interfaz ICE \texttt{mission} las maniobras de despegue y aterrizaje y los puntos de paso.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.396]{img/uavc.png}
  \caption{Pantalla principal de \texttt{UAV Commander}}
  \label{fig:uavc_ppal}
\end{figure}

\subsection{Creación y envío de misiones}

Para poder dar esta funcionalidad de una forma intuitiva y sencilla se han implementado varios ingredientes:
\begin{itemize}
\item Un mapa interactivo d\'onde un click crea un punto de misi\'on, dibujando un número secuencial que muestra el orden la misi\'on. Adem\'as, seg\'un van añadi\'endose nuevos puntos \texttt{UAV Commander} va dibujando la ruta que ``deber\'ia'' seguir el robot para el cumplimiento de su misi\'on.
Un simple click en el mapa interactivo desencadena:
\begin{enumerate}
\item Inclusi\'on del punto de paso en la lista de puntos de paso.
\item Llamada al m\'etodo de proyecci\'on \pythoninline{posImage2Coords(x, y, sizeX, sizeY, latMin, lonMin, latMax, lonMax)} para obtener a partir del x e y de la imagen la latitud y longitud del punto en el espacio a d\'onde se desea ir.
\item Control de misi\'on a realizar, verificando si se trata de un despegue, aterrizaje o un punto de paso intermedio.
\item Inserci\'on en la tabla de seguimiento de puntos de paso.
\item En la pr\'oxima iteraci\'on del threadMap a trav\'es de la llamada a \pythoninline{update_position()}.
\item Se llama al m\'etodo \pythoninline{setPosition(self, x, y,angle)} quien ejecuta el m\'etodo.
\item \pythoninline{set_waypoints(self, wayPoints, current=True)} quien pinta el nuevo punto de pasoy la ruta a seguir en el mapa.
\end{enumerate}
Las l\'ineas de c\'odigo de la figura 5.14 muestran la implementaci\'on del m\'etodo encargado de pintar los puntos de paso y las rutas en el mapa \pythoninline{set_waypoints(self, wayPoints, current=True)}
\begin{figure}
\begin{python}
    def set_waypoints(self, wayPoints, current=True):
        n=0
        pts = np.array(wayPoints, np.int32)
        pts = pts.reshape((-1, 1, 2))
        cv2.polylines(self.cvImageShadow, [pts], False, (250, 250, 250), thickness=1)
        for waypoint in wayPoints:
            n+=1
            s = str(n)
            cv2.circle(self.cvImageShadow, (waypoint[0], waypoint[1]), 1, [0, 0, 255], thickness=-1, lineType=1)
            cv2.putText(self.cvImageShadow, s, (waypoint[0] + 3, waypoint[1]), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, [0, 0, 255], thickness=1)
        self.refreshImage()
\end{python}
\caption{Método \texttt{set\_waypoints(self, wayPoints, current\=True)}, encargado de pintar los puntos de paso en el mapa}
\end{figure}
\item Bot\'on de ``Take Off''/``Land''. Este bot\'on apoya al mapa para crear misiones con aterrizaje y despegue, si se quiere despegar hacia un punto, basta con pulsarlo y pulsar despu\'es hacia d\'onde se quiere despegar. El comportamiento para el aterrizaje es an\'alogo.
\item Tabla de seguimiento de puntos de paso. En \'esta tabla se almacenan los puntos de paso que se van creando al hacer click sobre el mapa, cada fila es editable, de tal forma que ofrece una gran precisi\'on en caso que sea necesario.
\item Campo ``default altitude'' Establece la altitud de crucero, el valor de \'este campo se ir\'a propagando en cada punto de paso creado en el atributo \texttt{altitude}.
\item Bot\'on ``Send''. Este bot\'on llama al m\'etodo \pythoninline{sendWP(self, send2APM)} (Figura 5.15) quien recoge los objetos de misi\'on, los procesa y los env\'ia al bloque de comunicaci\'on con drivers también a trav\'es de los interfaces ICE necesarios, a saber, \texttt{mission} y \texttt{extra} en el caso en el que haya aterrizaje o despegue en la misi\'on.
\end{itemize}

\begin{figure}
\begin{python}
    def sendWP(self, send2APM):
    	...
        for row in range(colCount):
        	...
            pos_lat = text.find("lat:")
            pos_lon = text.find("lon:")
            if pos_lat != -1:
                lat = (text[pos_lat + 4:pos_lon])
                pose.x = float(lat)
                lon = (text[pos_lon + 4:])
                pose.y = float(lon)
            else:
                if "LAND in " in text:
                	...
                    self.extra.land()
                else:
                    pos = text.find("TAKE OFF to ") + 12
                    ...
                    self.extra.takeoff()
            alt = self.table.item(row, 1)
            pose.h = int(alt.text())
            mission.mission.append(pose)
            if i==0:
                mission.mission.append(pose)
            i += 1
\end{python}
\caption{Método \texttt{sendWP(self, send2APM)}, encargado de enviar la misión creada por el usuario al driver}
\end{figure}

\subsection{Control del vuelo}
El control se materializa en la placa estabilizadora a bordo del propio drone, en esta aplicación se visualiza su efecto, monitorizando en todo momento la posición 3D del avión y su relación con los puntos de paso de la misión.


Para llevar \'esto a cabo se ha dividido el mapa en tres capas:
\begin{enumerate}
\item Mapa original con el disclaimer. \pythoninline{self.cvImage}
\item Mapa original con la estela que ha dejado el robot a\'ereo. \pythoninline{self.cvImageShadow}
\item Mapa que va a mostrarse, donde se pinta la posici\'on actual del robot a\'ereo \pythoninline{self.im_to_show}
\end{enumerate}
Este planteamiento por capas permite, por ejemplo, mantener la estela del robot y pintar la posici\'on actual, añadir puntos de paso sin alterar lo ya dibujado, etc. Pero complica la implementaci\'on de todos los m\'etodos que interact\'uan en este bloque.
Algunos de los m\'etodos más importantes del desarrollo se encuentran aqu\'i y se listan a continuaci\'on:

\begin{itemize}
\item  \pythoninline{update_position(self)}
\item  \pythoninline{self.setPosition(self, x, y, angle)}
\item  \pythoninline{draw_triangle(self, x, y, angle)}
\item  \pythoninline{refresh_shadow(self,x,y)}
\item  \pythoninline{refreshImage(self)}
\end{itemize}

En cada iteraci\'on del hilo \texttt{threadMap} se articula el siguiente \'arbol con las llamadas:
\begin{itemize}
\item \pythoninline{update_position(self)}
	\begin{itemize}
	\item \pythoninline{GeoUtils.is_position_close_border(lat, lon , bbox)}
	\item \pythoninline{ImageUtils.posCoords2Image(lonMin, latMin, lonMax, latMax, lat, lon, tamImageX, tamImageY)}
	\item \pythoninline{setPosition(self, x, y, angle)}
		\begin{itemize}
		\item \pythoninline{refresh_shadow(self, x, y)}
		\item \pythoninline{set_wayPoints(self.wayPoints)}
		\item \pythoninline{draw_triangle(self, x, y, angle)}
			\begin{itemize}
			\item \pythoninline{GeoUtils.center_of_triangle(triangle)}
			\item \pythoninline{GeoUtils.change_coordinate_system(triangle, center,True)}
			\item \pythoninline{rotate_polygon(triangleCartesian,angle)}
			\end{itemize}
		\item \pythoninline{set_waypoints(self.wayPoints)}
		\item \pythoninline{self.refreshImage()}
		\end{itemize}
	\end{itemize}
\end{itemize}
En la Figura 5.16 se muestran las implementaciones de los m\'etodos con m\'as peso de \'este bloque.
\begin{figure}[H]
\begin{python}
    def update_position(self):

        if self.pose != None:
            pose = self.getPose3D().getPose3D()
            lat = pose.x
            lon = pose.y
            bbox = self.imageMetadata["bbox"]
            self.limit_warning = GeoUtils.is_position_close_border(lat, lon , bbox)
            if self.limit_warning:
                self.download_zoomed_map()
            else:
                bbox = self.imageMetadata["bbox"]
                imagesize = self.imageMetadata["size"]
                x, y = ImageUtils.posCoords2Image(bbox[0], bbox[1], bbox[2], bbox[3], lat, lon, imagesize[0], imagesize[1])
                angle = self.sensorsWidget.quatToYaw(pose.q0, pose.q1, pose.q2, pose.q3)
                self.setPosition(x, y, angle)

    def setPosition(self, x, y,angle):
        self.refresh_shadow(x,y)
        self.set_waypoints(self.wayPoints)
        self.draw_triangle(x, y, angle)
        self.refreshImage()

    def refresh_shadow(self,x,y):
            cv2.circle(self.cvImageShadow, (x, y), 1, [255,255,102], thickness=-1, lineType=8, shift=0)


\end{python}
\caption{Métodos encargados de actualizar la posición y el recorrido efectuado en el mapa}
\end{figure}




\cleardoublepage
\chapter{Experimentos}

En este capítulo se presentan las pruebas realizadas al sistema que proporcionan su validación experimental y el hardware necesario para la reproducción de las mismas. Estos experimentos han de validar los subobjetivos descritos en el Capítulo 2 y se dividen en nueve pruebas repartidas en dos bloques: experimentos en simulación y experimentos con el avión real.

\section{Experimentos con misiones en simulador}
\label{sec:experimentos_simulados}

En este apartado se presentan todos los experimentos realizados en simulación uno a uno, a saber:

\begin{enumerate}
\item Experimento de integración de todo el software.
\item Experimento de envío de misión con avión en vuelo.
\item Experimento de envío de misión con despegue y varios puntos de paso.
\item Experimento de envío de misión con despegue y aterrizaje.
\item Experimento de autozoom ante la salida del avión del mapa.
\end{enumerate}

\subsection{Experimento de integración de todo el software}

El objetivo de este experimento es probar que la integración del sistema y SITL se ha realizado correctamente. Vamos a verificar que se realiza correctamente la conexión con el drone simulado, y constatar que el driver recoge correctamente los datos sensoriales del drone. Se verifica también que estos datos se envían correctamente desde el driver a la aplicación y que ésta los representa fielmente. 
Para ello se han seguido los siguientes pasos:
\begin{enumerate}
\item Arranque y configuración de SITL.
\item Se carga una misión de prueba en el simulador a través de \texttt{MAVProxy}, donde se le ordena al drone seguir una serie de puntos de paso.
\item Se da comienzo a la misión simulada desde \texttt{MAVProxy}.
\item Se arranca el driver \texttt{APM Server}, es en este momento donde validaremos que se ha realizado la conexión con el drone simulado correctamente. 
\item Se arranca la aplicación \texttt{UAV Commander} y seguiremos la trayectoria del avión simulado a través del mapa de la aplicación, comprobando que pasa por los puntos de paso que hemos especificado en el simulador. Se comprobará que la ventana de actitud responde correctamente a los giros del drone simulado durante su misión.
\end{enumerate}

Se comprueba que funciona correctamente y se evidencia en el siguiente vídeo \url{https://www.youtube.com/watch?v=rHTi0buUETg}

\subsection{Experimento envío de misión con avión en vuelo}

El objetivo de este experimento es probar que el envío de misiones al drone simulado funciona correctamente y validar su cumplimiento. Para ello se van a realizar los siguiente pasos:
\begin{enumerate}
\item Arranque y configuración de SITL.
\item Se carga una misión de prueba a través de \texttt{MAVProxy}, en este caso a diferencia del caso anterior la misión tan solo consta de un despegue. Cuando el autopiloto comprueba que se han alcanzado todas las misiones enviadas entra en modo RTL \textit{}. En este modo el drone da vueltas trazando una circunferencia de un radio establecido por defecto en el \textit{firmware} de forma indefinida permitíendonos mandarle misiones estando en el aire.
\item Se da comienzo a la misión simulada desde \texttt{MAVProxy}.
\item Se arranca el driver \texttt{APM Server} conectando el driver al avion simulado.
\item Se arranca la aplicación \texttt{UAV Commander} conectándose con el driver.
\item Se crea una misión en \texttt{UAV Commander} haciendo click en los puntos del mapa interactivo por donde queramos que pase el avión.
\item Se envía la misión a \texttt{APM Server} a través del botón ``Send'' de \texttt{UAV Commander}
\end{enumerate}

En el experimento se comprueba que el avión ha seguido la ruta indicada desde \texttt{UAV Commander} correctamente.
La prueba se evidencia en el vídeo \url{https://www.youtube.com/watch?v=_X8WGSMIxug}

\subsection{Experimento de envío de misión con despegue y varios puntos de paso}
El objetivo de este experimento es probar que el envío de misiones y maniobra de despegue al avión simulado funciona correctamente y validar su cumplimiento. 

Seguiremos los mismos pasos que en el experimento anterior salvo por una particularidad, antes de clicar en el primer punto de paso (que será el primer punto al que vuele tras el despegue) pulsaremos el botón de ``Take Off''. De éste modo estaremos indicando al avión simulado <<Despega hacia el punto de paso X, Y>> donde X e Y serán latitud y longitud de la proyección del pixel de la imagen a coordenadas geográficas. 

Una vez realizada la secuencia arriba descrita se comprueba el seguimiento de la misión enviada a través del mapa, validando tanto que realiza la maniobra de despegue correctamente como el seguimiento de los puntos de paso. La prueba se evidencia en el vídeo \url{https://www.youtube.com/watch?v=0j3QttXIRbg}

\subsection{Experimento de envío de misión con despegue y aterrizaje}
El objetivo de este experimento es probar que el envío de misiones con maniobra de despegue y aterrizaje al avión simulado funciona correctamente y validar su cumplimiento.

Seguiremos los mismos pasos que en el experimento anterior salvo porque antes de clicar en el último punto de paso pulsaremos el botón ``Land''. De éste modo estaremos indicando al avión simulado <<Aterriza en el punto de paso X, Y>> donde X e Y serán latitud y longitud de la proyección del pixel de la imagen a coordenadas geográficas. 

Durante el experimento se puede comprobar que hay que ajustar muy bien la altitud del avión en los últimos puntos de paso, para que el avión aterrice en lo mas cerca posible del último. La prueba se evidencia en el vídeo \url{https://www.youtube.com/watch?v=4yf_PmFgR5Y}
\begin{figure}[H]
\centering{
   \label{f:diseno_uavc_caja_trans}
    \includegraphics[width=0.7\textwidth]{img/toff-land.png}}
    \caption{Seguimiento del experimento 6.1.4 dentro de la aplicación \texttt{UAV Commander}}
  
\end{figure}

\subsection{Experimento de autozoom ante la salida del avión del mapa}
El objetivo de este experimento es probar que el autozoom de \texttt{UAV Commander} funciona correctamente. 

Para ello se van a realizar los mismos pasos que en el experimento 6.1.1 estableciendo desde \texttt{MAVProxy} un punto de paso que quede claramente fuera del mapa descargado por defecto por \texttt{UAV Commander}.

Una vez realizada la secuencia arriba descrita se comprobará que \texttt{UAV Commander} descarga un nuevo mapa con una perspectiva mas amplia para seguir al avión. La prueba se evidencia en el vídeo \url{https://www.youtube.com/watch?v=vEcMbJoJ-Qo}

\begin{figure}[h]
\centering{
   \label{f:autozoom1}
    \includegraphics[width=0.483\textwidth]{img/antes.png}
   \label{f:autozoom2}
    \includegraphics[width=0.507\textwidth]{img/despues.png}
    \caption{Autozoom en UAV Commander}}
\end{figure}


\section{Interconexión del driver y aplicación con avión}
\label{sec:experimentos_real}

En este apartado se presentan todos los experimentos realizados con el avión real, a saber:

\begin{enumerate}
\item Experimento de recepción de vídeo.
\item Pruebas de alcance del enlace entre el avión simulado y el ordenador base.
\item Experimento de recepción de información sensorial y seguimiento del avión a través del \texttt{UAV Commander}.
\item Experimento de comportamiento ante pérdidas de señal.
\end{enumerate}

\subsection{Experimento de recepción de vídeo}
El objetivo de este experimento es verificar que el envío del vídeo capturado desde el ordenador de abordo con \texttt{cameraserver} se recibe correctamente en la aplicación \texttt{UAV Commander}. Para ello se van a realizar los siguiente pasos:

\begin{enumerate}
\item Arranque del ordenador de abordo.
\item Arranque del driver \texttt{cameraserver}.
\item Arranque de la aplicación \texttt{UAV Commander} donde clicaremos en el \textit{checkbox camera} y validaremos que se recibe correctamente el video transmitido.
\end{enumerate}
Esta prueba valida la recepción por parte de \texttt{AUV Commander} del vídeo que en este caso se emite desde otro driver distinto de \texttt{APM Server}. 
\begin{figure}[H]
\centering{
   \label{f:pruebavideo}
    \includegraphics[width=0.7\textwidth]{img/ventana_camera.png}}
    \caption{Prueba de recepción de vídeo desde cameraserver a \texttt{UAV Commander}}
  
\end{figure}

La prueba se evidencia en el vídeo \url{https://youtu.be/U-y8xstytuY}

\subsection{Pruebas de alcance del enlace entre el avión simulado y el ordenador base}
El objetivo de este experimento es probar que se reciben correctamente los datos de actitud del avión desde el ordenador base, según nos vamos alejando. Ésto nos delimitará el alcance al que vamos a volar. Para ello se van a realizar los siguiente pasos:
\begin{enumerate}
\item Arranque del avión a radiocontrol y del ordenador de abordo.
\item Arranque del driver \texttt{APM Server}, éste se conecta nada mas arrancar a la placa estabilizadora APM a través del puerto de serie.
\item Arranque de la aplicación \texttt{UAV Commander} quien se conecta a su vez al driver.
\item Se separará el avión del ordenador base hasta que se pierda la señal del wifi. De este modo sabremos la distancia máxima a la que podemos volar
\end{enumerate}
Este experimento nos va a permitir conocer a qué distancia vamos a poder volar. Este experimento se realizó dos veces: en la primera ocasión los datos arrojados por el experimento, apenas 10 metros, impedían cualquier tipo de vuelo del avión real. Con estos datos sobre la mesa se decidió adquirir un adaptador wifi mas potente para el ordenador de abordo, y se utilizó un \textit{router} con mas alcance para conectar ambos ordenadores. Con este nuevo hardware en una segunda prueba se alcanzó una distancia cercana a los 20 metros que permitiría un vuelo ceñido.
\begin{figure}[h]
\centering
  \subfloat[Alcance logrado en la primera prueba de alcance.]{
   \label{f:alcance1}
    \includegraphics[width=0.51\textwidth]{img/1.png}}
  \subfloat[Alcance logrado en la segunda prueba de alcance.]{
   \label{f:alcance2}
    \includegraphics[width=0.485\textwidth]{img/2.png}}
    \caption~{Resultados de las pruebas de alcance}
\end{figure}

\subsection{Experimento de recepción de información sensorial y seguimiento del avión a través del \texttt{UAV Commander}}
El objetivo de este experimento es probar que la integración del sistema y el avión real a radiocontrol se ha realizado correctamente. Se validará que se reciben correctamente los datos entregados por el driver y que se presentan fielmente a través del interfaz de usuario. Para ello se van a realizar los pasos del experimento anterior salvo que en este caso también moveremos el prototipo en sus 3 ejes de libertad para comprobar que se reciben y se presentan correctamente los datos en la ventana de actitud.

La prueba se evidencia en el vídeo \url{https://youtu.be/HFR7JjD-ZIs}

\subsection{Experimento de comportamiento ante pérdidas de señal}
El objetivo de este experimento es ver como se comporta \texttt{UAV Commander} ante la pérdida de la recepción de \texttt{APM Server}. A tenor de los datos obtenidos en el experimento 6.2.2 este experimento es imprescindible para conocer de antemano que puede pasar en un vuelo real.
Para llevarlo a cabo se simuló una pérdida de alcance apagando el adaptador wifi del ordenador base donde corre \texttt{UAV Commander}.
El resultado esperado era que tras la desconexión no se rompiera la ejecución de \texttt{APM Server} o de \texttt{UAV Commander} y se recuperase tras volver de nuevo a una zona de cobertura.
El experimento demuestra la robustez de ambos desarrollos ante pérdidas de señal, la evidencia puede verse en el vídeo \url{https://youtu.be/HFR7JjD-ZIs}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSIONES %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Conclusiones}
\label{chap:conclusiones}

Este PFC abre las puertas a futuros desarrollos con este tipo de robots aéreos, que hasta ahora JdeRobot no soportaba, sentando las bases del desarrollo de aplicaciones en JdeRobot para aviones autónomos. 
A lo largo del documento se han descrito los objetivos de este Proyecto Fin de Carrera, la plataforma hardware utilizada y la infraestructura software realizada para abordar el problema descrito en el capítulo dos. Se ha detallado el diseño e implementación de todos los desarrollos implementados en este proyecto, \texttt{APM Server} y \texttt{UAV Commander}, y se han validado con sendos experimentos en el capítulo seis.
El objetivo general de dar soporte en JdeRobot a drones tipo avión, incluyendo misiones como secuencia de puntos de paso se ha cumplido satisfactoriamente, al igual que los tres subobjetivos definidos en el capítulo dos. 



\section{Conclusiones}
\label{sec:conclusiones}



\begin{enumerate}

  \item El primer subojetivo del proyecto era el desarrollo de un driver para JdeRobot que nos permitiese conectarnos tanto al avión real como a uno simulado a través de SITL. El driver \texttt{APM Server} (ver capítulo 4) es capaz de conectarse tanto con aviones físicos que utilicen el protocolo de comunicación \texttt{MAVLink}, como con aviones simulados a través de SITL. El driver recupera los datos recogidos de los sensores del hardware de abordo del avión como el GPS y la IMU y los transforma en objetos que cumplen las interfaces ICE ya desarrolladas en JdeRobot \texttt{Pose3D} y \texttt{NavData}.


Para dar soporte a la navegación por posición se ha diseñado un nuevo interfaz, el interfaz \texttt{mission}, que junto con el \texttt{Exta} completan los interfaces que este driver recibe como órdenes y que se envían a través de los comandos \texttt{MAVLink} \texttt{MISSION\_ITEM}. El driver da soporte en actuación tanto a las maniobras de despegue y aterrizaje como a la navegación por puntos intermedios.

La concurrencia del componente permite que éste pueda responder a las peticiones de otros componentes mientras realiza otras acciones como la obtención de las imágenes del avión o el envío de órdenes de movimiento.

  \item El segundo subobjetivo era el desarrollo de una aplicaci\'on GCS \textit{Ground Control Station} que permita al operador humano introducir misiones y seguir el cumplimiento de las mismas a trav\'es de ella y el acceso a sus datos sensoriales. Para abordarlo se ha desarrollado la aplicación JdeRobot \texttt{UAV Commander}, para dar soporte al control de misiones en robots aéreos de ala fija, aviones. Esta aplicación muestra un interfaz gráfico en el cual el usuario humano puede visualizar los datos de los sensores a bordo (cámaras, posición, actitud), el mapa de la zona de trabajo del avión y componer gráficamente misiones de vuelo gracias al mapa interactivo. 

\texttt{UAV Commander} es capaz de descargar el mapa de la zona del mudo donde el avión se encuentra a través de WMS, y presentarlos por pantalla en forma de mapa interactivo. Clicando sobre el mapa se pueden crear de forma sencilla las misiones que el operador humano desee. Este mapa permite validar el cumplimiento de las misiones en vuelo gracias a elementos visuales añadidos como la ruta teórica a seguir o la estela dejada por el avión durante su vuelo. El control de misiones se complementa con una tabla donde se puede ver la secuencia de puntos de paso que se va a enviar al avión junto a la altura de vuelo. Esta tabla es editable para un ajuste mas exacto, y junto con los botones ``TakeOff/Land'', ``Clear'' y ``send'' completan el interfaz de creación de misiones.

\texttt{UAV Commander} ofrece toda la información sensorial del avión de forma visual en las ventanas de ``Camera'' y ``Attidude'', en ellas veremos los datos enviados por los sensores de abordo del avión como los acelerómetros, giróscopos o las cámaras de abordo.
La aplicación hace uso de los interfaces ICE desarrollados en JdeRobot, incluyendo el interfaz \texttt{mission} desarrollado íntegramente en este proyecto para dar soporte al control de misiones. Ésto permite que sea compatible con el resto de aplicaciones y drivers de JdeRobot y le confiere las ventajas de este tipo de desarrollos como la independencia del lenguaje de programación empleado o del hardware en que se esté ejecutando.

 \item El tercer subobjetivo era la validación experimental de ambos tanto en simulación como en el prototipo de avión real construido a tal efecto. Se ha validado el funcionamiento tanto de \texttt{APM Server} como de \texttt{UAV Comander} a través experimentos diseñados a tal efecto y descritos en el capítulo 6. Los experimentos han sido diseñados para ir validando de menos a más el correcto funcionamiento del driver y la aplicación tanto en un ambiente simulado (experimentos del 6.1.1 al 6.1.5) como en el prototipo de avión MAVLink (experimentos del 6.2.1 al 6.2.3)
 
En los experimentos con misiones en simulador se prueba de forma progresiva el comportamiento de \texttt{APM Server} y \texttt{UAV Commander} conectados a SITL. Empezamos comprobando la correcta conexión de \texttt{APM Server} a SITL, de \texttt{UAV Commander} a \texttt{APM Server} y que la aplicación \texttt{UAV Commander} presenta correctamente los datos en el experimento 6.1.1 donde queda además probada parte sensorial. En el experimento 6.1.2 se empieza a probar la actuación con el envió de puntos de paso y se le añaden progresivamente las maniobras de despegue y aterrizaje en los experimentos 6.1.3 y 6.1.4 respectivamente. El experimento 6.1.5 valida que \texttt{UAV Commander} no pierda el avión cuando éste se acerque al límite del mapa descargado.

En los experimentos de interconexión de driver y aplicación con el avión se comprueba la correcta recepción de toda la información sensorial(experimento 6.2.1 y 6.2.3). Tras probar en el experimento 6.2.2 que el alcance es mas limitado de lo esperado en un inicio, en el experimento 6.2.4 comprobamos la robustez de nuestros desarrollos ante una hipotética pérdida de señal al quedar fuera del alcance del wifi.


\end{enumerate}

El seguimiento y evolución de este PFC, así como el código, vídeos y material utilizado puede encontrarse en la web \url{http://jderobot.org/Jafernandez}


\section{Trabajos futuros}
\label{sec:trabajos_futuros}


Durante el desarrollo de \texttt{APM Server} y \texttt{UAV Commander} se han identificado varias evoluciones de los desarrollos aquí expuestos que mejorarían las funcionalidades actuales. Las describimos a continuación.


\begin{itemize}
	\item Se puede ampliar el soporte de misiones de \texttt{APM Server} hasta cubrir la totalidad de los commandos de misión de MAVLink. De esta forma se podría por ejemplo enviar misiones del tipo ``dar X vueltas de Y radio en torno al punto de paso Z'' o ``aumenta la velocidad durante la misión a X Km/h''.
	\item Se pueden ampliar en \texttt{APM Server} los interfaces para enviar información sobre la misión, el objetivo que he alcancado, el que estoy ejecutando, a qué distancia se encuentra del objetivo, etc.
	\item En \texttt{UAV Commander} se pueden mejorar los cálculos de proyección para que sean mas precisos independientemente de la zona del globo donde se encuentre, e independientemente del WSM de origen del mapa.
	\item En \texttt{UAV Commander} se puede ampliar la lista de orígenes de mapas añadiendo los WMS de \texttt{OpenStreetMap}, \texttt{Yahoo Maps} y \texttt{Bing Maps} .
	\item En \texttt{UAV Commander} se puede añadir el control por velocidades \texttt{CMDVel} para dar soporte a todo tipo de drones. Y que esta aplicación se pudiera emplear con drones tipo cuadricóptero, capaces de cernirse en un sitio fijo y obedecer órdenes de velocidad en varios ejes.
	\item Como desarrollo adicional se podría conectar SITL a Gazebo, de ésta forma tendríamos la simulación completa incluyendo el sensor de cámara y se podrían probar las misiones en entornos más realistas. Esto facilitaría la simulación de desarrollos que utilicen la visión en aviones. 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAFIA %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage

% Las siguientes dos instrucciones es todo lo que necesitas
% para incluir las citas en la memoria
\bibliographystyle{abbrv}
\bibliography{memoria}  % memoria.bib es el nombre del fichero que contiene
% las referencias bibliogr\'aficas. Abre ese fichero y mira el formato que tiene,
% que se conoce como BibTeX. Hay muchos sitios que exportan referencias en
% formato BibTeX. Prueba a buscar en http://scholar.google.com por referencias
% y ver\'as que lo puedes hacer de manera sencilla.
% M\'as informaci\'on: 
% http://texblog.org/2014/04/22/using-google-scholar-to-download-bibtex-citations/

\end{document}
\grid
\grid
\grid
\grid
\grid
